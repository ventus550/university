{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 122348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from word_emb_evaluation import words, ABX\n",
    "from utils import load_model, configure_environment\n",
    "\n",
    "\n",
    "configure_environment(device=\"cuda\")\n",
    "bert, bert_tokenizer, device = load_model(model_name=\"allegro/herbert-base-cased\")\n",
    "papuga, papuga_tokenizer, device = load_model(model_name=\"flax-community/papuGaPT2\", causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def papuga_word_embedding(word):\n",
    "\tdef papuga_tokenize(word):\n",
    "\t\tids = papuga_tokenizer(word, return_tensors='pt')['input_ids'][0]\n",
    "\t\treturn [papuga_tokenizer.decode(n) for n in ids]\n",
    "\n",
    "\temb = papuga.transformer.wte.weight.detach().cpu().numpy()\n",
    "\n",
    "\ttokens = papuga_tokenize(word)\n",
    "\treturn torch.tensor(emb[[papuga_tokenizer.encode(token) for token in tokens]], dtype=float).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ffa07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_word_embedding(word):\n",
    "    input_ids = bert_tokenizer(word, return_tensors='pt')['input_ids'] #.to(device)\n",
    "    output = bert(input_ids=input_ids)\n",
    "    return output.last_hidden_state[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "131b943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "papuga_wembedds = {word: papuga_word_embedding(word) for word in words}\n",
    "bert_wembedds = {word: bert_word_embedding(word) for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7510f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.58707\n"
     ]
    }
   ],
   "source": [
    "ABX(papuga_wembedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d602b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.591602\n"
     ]
    }
   ],
   "source": [
    "ABX(bert_wembedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a058349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_swap(word):\n",
    "\tidx1, idx2 = random.sample(range(len(word)), 2)\n",
    "\tword = list(word)\n",
    "\tword[idx1], word[idx2] = word[idx2], word[idx1]\n",
    "\treturn \"\".join(word)\n",
    "\n",
    "swapped_bert_wembedds = {word: bert_word_embedding(character_swap(word)) for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91cf2687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.527092\n"
     ]
    }
   ],
   "source": [
    "ABX(swapped_bert_wembedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "470efe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwerty_neighbors = {\n",
    "    'q': ['w', 'a'], 'w': ['q', 'e', 'a', 's'], 'e': ['w', 'r', 's', 'd'],\n",
    "    'r': ['e', 't', 'd', 'f'], 't': ['r', 'y', 'f', 'g'], 'y': ['t', 'u', 'g', 'h'],\n",
    "    'u': ['y', 'i', 'h', 'j'], 'i': ['u', 'o', 'j', 'k'], 'o': ['i', 'p', 'k', 'l'],\n",
    "    'p': ['o', 'l'], 'a': ['q', 'w', 's', 'z'], 's': ['a', 'w', 'e', 'z', 'x'],\n",
    "    'd': ['s', 'e', 'r', 'x', 'c'], 'f': ['d', 'r', 't', 'c', 'v'], 'g': ['f', 't', 'y', 'v', 'b'],\n",
    "    'h': ['g', 'y', 'u', 'b', 'n'], 'j': ['h', 'u', 'i', 'n', 'm'], 'k': ['j', 'i', 'o', 'm'],\n",
    "    'l': ['k', 'o', 'p'], 'z': ['a', 's', 'x'], 'x': ['z', 's', 'd', 'c'], 'c': ['x', 'd', 'f', 'v'],\n",
    "    'v': ['c', 'f', 'g', 'b'], 'b': ['v', 'g', 'h', 'n'], 'n': ['b', 'h', 'j', 'm'],\n",
    "    'm': ['n', 'j', 'k']\n",
    "}\n",
    "\n",
    "def random_qwerty_swap(word, swap_count=1):\n",
    "    # Convert the word to a list of characters for mutability\n",
    "    word_chars = list(word)\n",
    "    indices = list(range(len(word_chars)))\n",
    "    random.shuffle(indices)  # Shuffle indices to choose random positions\n",
    "\n",
    "    for _ in range(min(swap_count, len(word))):  # Ensure we don't exceed the word length\n",
    "        idx = indices.pop()  # Take a random index\n",
    "        char = word_chars[idx].lower()  # Get the character and make it lowercase\n",
    "        if char in qwerty_neighbors:  # Check if the char is in our mapping\n",
    "            replacement = random.choice(qwerty_neighbors[char])\n",
    "            word_chars[idx] = replacement\n",
    "\n",
    "    return ''.join(word_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71cd4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwerty_bert_wembedds = {word: bert_word_embedding(random_qwerty_swap(word)) for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "828a1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEMS: 0.0\n",
      "Start\n",
      "TOTAL SCORE: 0.528666\n"
     ]
    }
   ],
   "source": [
    "ABX(qwerty_bert_wembedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164156e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
