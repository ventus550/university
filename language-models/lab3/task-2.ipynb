{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202d3525",
   "metadata": {},
   "source": [
    "### Representation of Contextualized Embeddings\n",
    "1. Each position in the last hidden state corresponds to the embedding of a token in the input sequence.\n",
    "2. These embeddings are contextualized, meaning they encode information about the token itself as well as its relationships and dependencies with other tokens in the sequence.\n",
    "3. For example, the word \"bank\" in the context of \"river bank\" and \"financial bank\" will have different embeddings in the last hidden state because the model incorporates the surrounding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 8610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from utils import load_model, load_review_data, configure_environment\n",
    "\n",
    "configure_environment(device=\"cuda\")\n",
    "bert, bert_tokenizer, device = load_model(model_name=\"allegro/herbert-base-cased\")\n",
    "papuga, papuga_tokenizer, device = load_model(model_name=\"flax-community/papuGaPT2\", causal=True)\n",
    "reviews_df = load_review_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95caf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-38.80006408691406, (768,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representation(txt):\n",
    "    input_ids = bert_tokenizer(txt, return_tensors='pt')['input_ids'] #.to(device)\n",
    "    output = bert(input_ids=input_ids)\n",
    "    return output.last_hidden_state.detach().cpu().numpy()[0,0,:]\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sentence_prob(sentence_txt):\n",
    "    suffix = \" Opinia jest pozytywna.\"\n",
    "    input_ids = papuga_tokenizer(sentence_txt + suffix, return_tensors='pt')['input_ids'] #.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = papuga(input_ids=input_ids)\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs)\n",
    "    return seq_log_probs.cpu().numpy().item()\n",
    "\n",
    "sentence_prob(\"ala ma kota\"), representation(\"ala ma kota\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee94767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "\tdf = df.copy().join(df.text.apply(representation).apply(pd.Series).add_prefix('features.bert.'))\n",
    "\tdf[\"features.papuga.probability\"] = df.text.map(sentence_prob)\n",
    "\tdf.columns = pd.MultiIndex.from_tuples([col.split('.') for col in df.columns])\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128afedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_features_df = extract_features(train_df)\n",
    "test_features_df = extract_features(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca84e9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th colspan=\"19\" halign=\"left\">features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th colspan=\"18\" halign=\"left\">bert</th>\n",
       "      <th>papuga</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>Wypożyczalnia samochodów w firmie hotelowej, r...</td>\n",
       "      <td>-0.397389</td>\n",
       "      <td>-0.312147</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.142620</td>\n",
       "      <td>-0.497406</td>\n",
       "      <td>1.163807</td>\n",
       "      <td>-0.103312</td>\n",
       "      <td>-0.083231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506825</td>\n",
       "      <td>-0.065348</td>\n",
       "      <td>0.255681</td>\n",
       "      <td>0.152758</td>\n",
       "      <td>-0.118751</td>\n",
       "      <td>0.277989</td>\n",
       "      <td>-0.122502</td>\n",
       "      <td>-0.009789</td>\n",
       "      <td>-0.320787</td>\n",
       "      <td>-62.358536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>Polecam ten hotel - świetny widok, super dojazd.</td>\n",
       "      <td>-0.208168</td>\n",
       "      <td>-0.021141</td>\n",
       "      <td>0.051543</td>\n",
       "      <td>0.126063</td>\n",
       "      <td>0.110358</td>\n",
       "      <td>0.305206</td>\n",
       "      <td>0.052856</td>\n",
       "      <td>-0.283542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468360</td>\n",
       "      <td>-0.024745</td>\n",
       "      <td>0.465612</td>\n",
       "      <td>0.056653</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>0.065137</td>\n",
       "      <td>0.244588</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.404718</td>\n",
       "      <td>-57.420876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>False</td>\n",
       "      <td>Apartamenty znajdują się w budynkach, które po...</td>\n",
       "      <td>-0.213159</td>\n",
       "      <td>-0.214663</td>\n",
       "      <td>0.228404</td>\n",
       "      <td>0.289841</td>\n",
       "      <td>-0.148720</td>\n",
       "      <td>0.278599</td>\n",
       "      <td>0.069608</td>\n",
       "      <td>-0.246748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038890</td>\n",
       "      <td>-0.050254</td>\n",
       "      <td>0.229985</td>\n",
       "      <td>0.074232</td>\n",
       "      <td>-0.100421</td>\n",
       "      <td>0.301730</td>\n",
       "      <td>0.017435</td>\n",
       "      <td>0.059403</td>\n",
       "      <td>0.427140</td>\n",
       "      <td>-82.344879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>False</td>\n",
       "      <td>POZDRAWIAM PANI DOKTOR ; - /</td>\n",
       "      <td>-0.132632</td>\n",
       "      <td>0.047073</td>\n",
       "      <td>0.012956</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>-0.293542</td>\n",
       "      <td>1.047164</td>\n",
       "      <td>-0.104890</td>\n",
       "      <td>-0.061957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773239</td>\n",
       "      <td>-0.040484</td>\n",
       "      <td>0.440073</td>\n",
       "      <td>0.010412</td>\n",
       "      <td>0.204703</td>\n",
       "      <td>-0.235704</td>\n",
       "      <td>-0.174118</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>0.276662</td>\n",
       "      <td>-67.415474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>False</td>\n",
       "      <td>Pokoje nie są sprzątane.</td>\n",
       "      <td>0.028177</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.050209</td>\n",
       "      <td>0.224215</td>\n",
       "      <td>0.238290</td>\n",
       "      <td>0.050711</td>\n",
       "      <td>-0.197451</td>\n",
       "      <td>-0.415037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318681</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>0.346750</td>\n",
       "      <td>0.063330</td>\n",
       "      <td>0.027019</td>\n",
       "      <td>0.344931</td>\n",
       "      <td>-0.301065</td>\n",
       "      <td>-0.050593</td>\n",
       "      <td>0.266963</td>\n",
       "      <td>-32.901093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>True</td>\n",
       "      <td>Wielokrotnie pomagał moim dzieciom Oldze i Jus...</td>\n",
       "      <td>-0.077440</td>\n",
       "      <td>-0.058589</td>\n",
       "      <td>-0.010326</td>\n",
       "      <td>0.023113</td>\n",
       "      <td>-0.568084</td>\n",
       "      <td>0.369323</td>\n",
       "      <td>-0.064523</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334429</td>\n",
       "      <td>0.117695</td>\n",
       "      <td>0.189385</td>\n",
       "      <td>0.391590</td>\n",
       "      <td>-0.150924</td>\n",
       "      <td>0.213030</td>\n",
       "      <td>0.150356</td>\n",
       "      <td>0.086259</td>\n",
       "      <td>-0.618287</td>\n",
       "      <td>-136.673615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>True</td>\n",
       "      <td>Jedyny lekarz, który faktycznie szukał rozwiązań.</td>\n",
       "      <td>-0.095759</td>\n",
       "      <td>0.063175</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>-0.064605</td>\n",
       "      <td>-0.393593</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>0.074060</td>\n",
       "      <td>0.070047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189531</td>\n",
       "      <td>-0.031423</td>\n",
       "      <td>0.299835</td>\n",
       "      <td>0.107138</td>\n",
       "      <td>-0.126691</td>\n",
       "      <td>0.284013</td>\n",
       "      <td>-0.187929</td>\n",
       "      <td>0.192120</td>\n",
       "      <td>0.066619</td>\n",
       "      <td>-53.901554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>False</td>\n",
       "      <td>Kolejny to problem z talerzami i sztućcami i s...</td>\n",
       "      <td>0.064061</td>\n",
       "      <td>0.143702</td>\n",
       "      <td>-0.038295</td>\n",
       "      <td>0.140301</td>\n",
       "      <td>0.332156</td>\n",
       "      <td>-0.135974</td>\n",
       "      <td>-0.097481</td>\n",
       "      <td>-0.152080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.035912</td>\n",
       "      <td>0.428588</td>\n",
       "      <td>0.278874</td>\n",
       "      <td>-0.135661</td>\n",
       "      <td>-0.123172</td>\n",
       "      <td>-0.252835</td>\n",
       "      <td>0.089340</td>\n",
       "      <td>-0.238518</td>\n",
       "      <td>-158.545105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>False</td>\n",
       "      <td>Znajomi zaproponowali, żebym zostawił bagaże i...</td>\n",
       "      <td>0.201128</td>\n",
       "      <td>-0.082920</td>\n",
       "      <td>0.134153</td>\n",
       "      <td>-0.005217</td>\n",
       "      <td>0.176424</td>\n",
       "      <td>0.418421</td>\n",
       "      <td>-0.101599</td>\n",
       "      <td>-0.272516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588318</td>\n",
       "      <td>0.112465</td>\n",
       "      <td>0.249527</td>\n",
       "      <td>0.143351</td>\n",
       "      <td>-0.177011</td>\n",
       "      <td>0.529219</td>\n",
       "      <td>-0.156516</td>\n",
       "      <td>0.140669</td>\n",
       "      <td>-0.058572</td>\n",
       "      <td>-253.174667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>True</td>\n",
       "      <td>Jestem pewny, że to najlepszy dermatolog w War...</td>\n",
       "      <td>-0.297178</td>\n",
       "      <td>0.108536</td>\n",
       "      <td>-0.102323</td>\n",
       "      <td>0.077380</td>\n",
       "      <td>-0.239001</td>\n",
       "      <td>0.452057</td>\n",
       "      <td>0.026686</td>\n",
       "      <td>0.445728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642748</td>\n",
       "      <td>0.171744</td>\n",
       "      <td>0.207117</td>\n",
       "      <td>0.405344</td>\n",
       "      <td>-0.132598</td>\n",
       "      <td>0.172701</td>\n",
       "      <td>-0.085032</td>\n",
       "      <td>0.209558</td>\n",
       "      <td>0.483075</td>\n",
       "      <td>-44.367123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  features  \\\n",
       "       NaN                                                NaN      bert   \n",
       "       NaN                                                NaN         0   \n",
       "3     True  Wypożyczalnia samochodów w firmie hotelowej, r... -0.397389   \n",
       "18    True   Polecam ten hotel - świetny widok, super dojazd. -0.208168   \n",
       "202  False  Apartamenty znajdują się w budynkach, które po... -0.213159   \n",
       "250  False                       POZDRAWIAM PANI DOKTOR ; - / -0.132632   \n",
       "274  False                           Pokoje nie są sprzątane.  0.028177   \n",
       "..     ...                                                ...       ...   \n",
       "71    True  Wielokrotnie pomagał moim dzieciom Oldze i Jus... -0.077440   \n",
       "106   True  Jedyny lekarz, który faktycznie szukał rozwiązań. -0.095759   \n",
       "270  False  Kolejny to problem z talerzami i sztućcami i s...  0.064061   \n",
       "348  False  Znajomi zaproponowali, żebym zostawił bagaże i...  0.201128   \n",
       "102   True  Jestem pewny, że to najlepszy dermatolog w War... -0.297178   \n",
       "\n",
       "                                                                           \\\n",
       "                                                                            \n",
       "            1         2         3         4         5         6         7   \n",
       "3   -0.312147  0.056186  0.142620 -0.497406  1.163807 -0.103312 -0.083231   \n",
       "18  -0.021141  0.051543  0.126063  0.110358  0.305206  0.052856 -0.283542   \n",
       "202 -0.214663  0.228404  0.289841 -0.148720  0.278599  0.069608 -0.246748   \n",
       "250  0.047073  0.012956  0.115169 -0.293542  1.047164 -0.104890 -0.061957   \n",
       "274  0.019898  0.050209  0.224215  0.238290  0.050711 -0.197451 -0.415037   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "71  -0.058589 -0.010326  0.023113 -0.568084  0.369323 -0.064523  0.011742   \n",
       "106  0.063175  0.011771 -0.064605 -0.393593  0.514520  0.074060  0.070047   \n",
       "270  0.143702 -0.038295  0.140301  0.332156 -0.135974 -0.097481 -0.152080   \n",
       "348 -0.082920  0.134153 -0.005217  0.176424  0.418421 -0.101599 -0.272516   \n",
       "102  0.108536 -0.102323  0.077380 -0.239001  0.452057  0.026686  0.445728   \n",
       "\n",
       "     ...                                                              \\\n",
       "     ...                                                               \n",
       "     ...       759       760       761       762       763       764   \n",
       "3    ...  0.506825 -0.065348  0.255681  0.152758 -0.118751  0.277989   \n",
       "18   ...  0.468360 -0.024745  0.465612  0.056653 -0.000556  0.065137   \n",
       "202  ... -0.038890 -0.050254  0.229985  0.074232 -0.100421  0.301730   \n",
       "250  ...  0.773239 -0.040484  0.440073  0.010412  0.204703 -0.235704   \n",
       "274  ... -0.318681  0.055497  0.346750  0.063330  0.027019  0.344931   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "71   ...  0.334429  0.117695  0.189385  0.391590 -0.150924  0.213030   \n",
       "106  ...  0.189531 -0.031423  0.299835  0.107138 -0.126691  0.284013   \n",
       "270  ...  0.267881  0.035912  0.428588  0.278874 -0.135661 -0.123172   \n",
       "348  ... -0.588318  0.112465  0.249527  0.143351 -0.177011  0.529219   \n",
       "102  ...  0.642748  0.171744  0.207117  0.405344 -0.132598  0.172701   \n",
       "\n",
       "                                               \n",
       "                                       papuga  \n",
       "          765       766       767 probability  \n",
       "3   -0.122502 -0.009789 -0.320787  -62.358536  \n",
       "18   0.244588  0.093530  0.404718  -57.420876  \n",
       "202  0.017435  0.059403  0.427140  -82.344879  \n",
       "250 -0.174118  0.060988  0.276662  -67.415474  \n",
       "274 -0.301065 -0.050593  0.266963  -32.901093  \n",
       "..        ...       ...       ...         ...  \n",
       "71   0.150356  0.086259 -0.618287 -136.673615  \n",
       "106 -0.187929  0.192120  0.066619  -53.901554  \n",
       "270 -0.252835  0.089340 -0.238518 -158.545105  \n",
       "348 -0.156516  0.140669 -0.058572 -253.174667  \n",
       "102 -0.085032  0.209558  0.483075  -44.367123  \n",
       "\n",
       "[320 rows x 771 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803d8d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train_features_df.features.bert.values, train_features_df.label.squeeze())\n",
    "\n",
    "print ('Train accuracy:', clf.score(train_features_df.features.bert.values, train_features_df.label.squeeze()))\n",
    "print ('Test accuracy:', clf.score(test_features_df.features.bert.values, test_features_df.label.squeeze()))\n",
    "\n",
    "#Train accuracy: 0.9348939283101683\n",
    "#Test accuracy: 0.8715697036223929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e6368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a96e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
