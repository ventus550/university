{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e01f65",
   "metadata": {},
   "source": [
    "### Load gensim model from [github](https://github.com/sdadas/polish-nlp-resources/releases/download/v1.0/word2vec.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf1dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from utils import load_model, load_review_data, configure_environment, logistic_regression, augment_data\n",
    "\n",
    "configure_environment()\n",
    "bert, bert_tokenizer, device = load_model(model_name=\"allegro/herbert-base-cased\")\n",
    "reviews_df = load_review_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95caf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(txt):\n",
    "    input_ids = bert_tokenizer(txt, return_tensors='pt')['input_ids']\n",
    "    output = bert(input_ids=input_ids)\n",
    "    return output.last_hidden_state.detach().cpu().numpy()[0,0,:]\n",
    "\n",
    "def extract_features(df):\n",
    "\tdf = df.copy().join(df.text.apply(representation).apply(pd.Series).add_prefix('features.bert.'))\n",
    "\tdf.columns = pd.MultiIndex.from_tuples([col.split('.') for col in df.columns])\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128afedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(reviews_df, test_size=0.2, shuffle=True)\n",
    "train_features_df = extract_features(train_df)\n",
    "test_features_df = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04944dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th colspan=\"19\" halign=\"left\">features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th colspan=\"19\" halign=\"left\">bert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "      <td>Jestem 2 dni po zabiegu i naprawde widac popra...</td>\n",
       "      <td>-0.166789</td>\n",
       "      <td>0.055120</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.062562</td>\n",
       "      <td>0.038650</td>\n",
       "      <td>-0.191555</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.030428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061630</td>\n",
       "      <td>0.284108</td>\n",
       "      <td>0.106787</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.153379</td>\n",
       "      <td>-0.102755</td>\n",
       "      <td>-0.086485</td>\n",
       "      <td>0.057876</td>\n",
       "      <td>0.175473</td>\n",
       "      <td>-0.210865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>False</td>\n",
       "      <td>Nieprofesjonalna obsługa w barze ( pozdrowieni...</td>\n",
       "      <td>0.221364</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>-0.108641</td>\n",
       "      <td>0.176262</td>\n",
       "      <td>0.096322</td>\n",
       "      <td>0.067469</td>\n",
       "      <td>-0.090967</td>\n",
       "      <td>-0.078779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011472</td>\n",
       "      <td>-0.204341</td>\n",
       "      <td>0.049946</td>\n",
       "      <td>0.091792</td>\n",
       "      <td>0.351829</td>\n",
       "      <td>-0.085705</td>\n",
       "      <td>0.089164</td>\n",
       "      <td>-0.852266</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>-0.039202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>False</td>\n",
       "      <td>Pytania podczas laborki potrafią byc ciężkie, ...</td>\n",
       "      <td>-0.076753</td>\n",
       "      <td>-0.078704</td>\n",
       "      <td>-0.138457</td>\n",
       "      <td>0.265353</td>\n",
       "      <td>-0.016553</td>\n",
       "      <td>0.161608</td>\n",
       "      <td>0.042638</td>\n",
       "      <td>-0.196528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122220</td>\n",
       "      <td>0.571785</td>\n",
       "      <td>-0.126486</td>\n",
       "      <td>0.054753</td>\n",
       "      <td>0.216552</td>\n",
       "      <td>0.133295</td>\n",
       "      <td>-0.279513</td>\n",
       "      <td>-0.523560</td>\n",
       "      <td>0.094154</td>\n",
       "      <td>-0.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>False</td>\n",
       "      <td>Po otwarciu drzwi pokoju uderzał nieprzyjemny ...</td>\n",
       "      <td>-0.204082</td>\n",
       "      <td>-0.040308</td>\n",
       "      <td>-0.048479</td>\n",
       "      <td>0.083144</td>\n",
       "      <td>-0.079823</td>\n",
       "      <td>0.166468</td>\n",
       "      <td>0.070079</td>\n",
       "      <td>0.083498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059195</td>\n",
       "      <td>-0.102907</td>\n",
       "      <td>-0.007124</td>\n",
       "      <td>0.212242</td>\n",
       "      <td>0.292041</td>\n",
       "      <td>-0.137885</td>\n",
       "      <td>-0.428366</td>\n",
       "      <td>0.086692</td>\n",
       "      <td>-0.039449</td>\n",
       "      <td>0.570365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>False</td>\n",
       "      <td>Brak lobby i miejsc do wypoczynku.</td>\n",
       "      <td>0.198214</td>\n",
       "      <td>-0.231528</td>\n",
       "      <td>-0.159114</td>\n",
       "      <td>-0.029486</td>\n",
       "      <td>0.636929</td>\n",
       "      <td>0.149423</td>\n",
       "      <td>-0.089068</td>\n",
       "      <td>0.492712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048487</td>\n",
       "      <td>-0.893390</td>\n",
       "      <td>-0.119268</td>\n",
       "      <td>0.101796</td>\n",
       "      <td>0.220019</td>\n",
       "      <td>-0.299325</td>\n",
       "      <td>-0.456732</td>\n",
       "      <td>0.276496</td>\n",
       "      <td>-0.105577</td>\n",
       "      <td>-0.123156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>True</td>\n",
       "      <td>Pokoje te są zlokalizowane na piętrze pensjona...</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>-0.020467</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.219762</td>\n",
       "      <td>0.223317</td>\n",
       "      <td>0.273472</td>\n",
       "      <td>-0.226220</td>\n",
       "      <td>-0.408401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183351</td>\n",
       "      <td>-0.326955</td>\n",
       "      <td>-0.013456</td>\n",
       "      <td>0.384032</td>\n",
       "      <td>0.095777</td>\n",
       "      <td>-0.041340</td>\n",
       "      <td>0.287313</td>\n",
       "      <td>-0.191112</td>\n",
       "      <td>0.084220</td>\n",
       "      <td>0.213388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>True</td>\n",
       "      <td>Wprowadza atmosferę spokoju i rzeczowości.</td>\n",
       "      <td>0.135019</td>\n",
       "      <td>-0.009740</td>\n",
       "      <td>-0.152579</td>\n",
       "      <td>0.189626</td>\n",
       "      <td>0.323342</td>\n",
       "      <td>0.124945</td>\n",
       "      <td>-0.287877</td>\n",
       "      <td>-0.365972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216853</td>\n",
       "      <td>0.231571</td>\n",
       "      <td>-0.034450</td>\n",
       "      <td>-0.047848</td>\n",
       "      <td>0.212989</td>\n",
       "      <td>-0.165746</td>\n",
       "      <td>-0.020718</td>\n",
       "      <td>-0.154120</td>\n",
       "      <td>0.285610</td>\n",
       "      <td>-0.186035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>False</td>\n",
       "      <td>to sa te trafne zaściankowe diagnozy.</td>\n",
       "      <td>0.190267</td>\n",
       "      <td>0.058012</td>\n",
       "      <td>-0.012369</td>\n",
       "      <td>0.163534</td>\n",
       "      <td>-0.248350</td>\n",
       "      <td>-0.255285</td>\n",
       "      <td>-0.112946</td>\n",
       "      <td>-0.038393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028444</td>\n",
       "      <td>0.193244</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>0.276538</td>\n",
       "      <td>0.256483</td>\n",
       "      <td>-0.142741</td>\n",
       "      <td>-0.084024</td>\n",
       "      <td>-0.085960</td>\n",
       "      <td>-0.028197</td>\n",
       "      <td>-0.391466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>False</td>\n",
       "      <td>Na pewno nikomu nie polecam tego hotelu!</td>\n",
       "      <td>-0.278896</td>\n",
       "      <td>-0.066436</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>0.145937</td>\n",
       "      <td>0.137390</td>\n",
       "      <td>0.357342</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>-0.064014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101406</td>\n",
       "      <td>-0.207535</td>\n",
       "      <td>-0.029110</td>\n",
       "      <td>0.360082</td>\n",
       "      <td>0.195425</td>\n",
       "      <td>0.043408</td>\n",
       "      <td>-0.282126</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>-0.107082</td>\n",
       "      <td>0.115247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>False</td>\n",
       "      <td>Wydałem 150zl plus dojazd.</td>\n",
       "      <td>-0.289713</td>\n",
       "      <td>0.144982</td>\n",
       "      <td>0.052612</td>\n",
       "      <td>-0.064344</td>\n",
       "      <td>-0.058165</td>\n",
       "      <td>-0.039919</td>\n",
       "      <td>-0.041845</td>\n",
       "      <td>-0.382918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166939</td>\n",
       "      <td>-0.105442</td>\n",
       "      <td>0.101909</td>\n",
       "      <td>0.253416</td>\n",
       "      <td>0.244875</td>\n",
       "      <td>-0.091224</td>\n",
       "      <td>-0.127410</td>\n",
       "      <td>-0.465330</td>\n",
       "      <td>0.109324</td>\n",
       "      <td>0.090790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  features  \\\n",
       "       NaN                                                NaN      bert   \n",
       "       NaN                                                NaN         0   \n",
       "112   True  Jestem 2 dni po zabiegu i naprawde widac popra... -0.166789   \n",
       "365  False  Nieprofesjonalna obsługa w barze ( pozdrowieni...  0.221364   \n",
       "339  False  Pytania podczas laborki potrafią byc ciężkie, ... -0.076753   \n",
       "329  False  Po otwarciu drzwi pokoju uderzał nieprzyjemny ... -0.204082   \n",
       "286  False                 Brak lobby i miejsc do wypoczynku.  0.198214   \n",
       "..     ...                                                ...       ...   \n",
       "50    True  Pokoje te są zlokalizowane na piętrze pensjona...  0.043725   \n",
       "97    True         Wprowadza atmosferę spokoju i rzeczowości.  0.135019   \n",
       "355  False              to sa te trafne zaściankowe diagnozy.  0.190267   \n",
       "388  False           Na pewno nikomu nie polecam tego hotelu! -0.278896   \n",
       "282  False                         Wydałem 150zl plus dojazd. -0.289713   \n",
       "\n",
       "                                                                           \\\n",
       "                                                                            \n",
       "            1         2         3         4         5         6         7   \n",
       "112  0.055120  0.040738  0.062562  0.038650 -0.191555  0.001619  0.030428   \n",
       "365 -0.029767 -0.108641  0.176262  0.096322  0.067469 -0.090967 -0.078779   \n",
       "339 -0.078704 -0.138457  0.265353 -0.016553  0.161608  0.042638 -0.196528   \n",
       "329 -0.040308 -0.048479  0.083144 -0.079823  0.166468  0.070079  0.083498   \n",
       "286 -0.231528 -0.159114 -0.029486  0.636929  0.149423 -0.089068  0.492712   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "50  -0.020467  0.043737  0.219762  0.223317  0.273472 -0.226220 -0.408401   \n",
       "97  -0.009740 -0.152579  0.189626  0.323342  0.124945 -0.287877 -0.365972   \n",
       "355  0.058012 -0.012369  0.163534 -0.248350 -0.255285 -0.112946 -0.038393   \n",
       "388 -0.066436  0.084914  0.145937  0.137390  0.357342  0.008929 -0.064014   \n",
       "282  0.144982  0.052612 -0.064344 -0.058165 -0.039919 -0.041845 -0.382918   \n",
       "\n",
       "     ...                                                              \\\n",
       "     ...                                                               \n",
       "     ...       758       759       760       761       762       763   \n",
       "112  ...  0.061630  0.284108  0.106787  0.163743  0.153379 -0.102755   \n",
       "365  ...  0.011472 -0.204341  0.049946  0.091792  0.351829 -0.085705   \n",
       "339  ...  0.122220  0.571785 -0.126486  0.054753  0.216552  0.133295   \n",
       "329  ...  0.059195 -0.102907 -0.007124  0.212242  0.292041 -0.137885   \n",
       "286  ...  0.048487 -0.893390 -0.119268  0.101796  0.220019 -0.299325   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "50   ...  0.183351 -0.326955 -0.013456  0.384032  0.095777 -0.041340   \n",
       "97   ... -0.216853  0.231571 -0.034450 -0.047848  0.212989 -0.165746   \n",
       "355  ... -0.028444  0.193244 -0.025528  0.276538  0.256483 -0.142741   \n",
       "388  ... -0.101406 -0.207535 -0.029110  0.360082  0.195425  0.043408   \n",
       "282  ...  0.166939 -0.105442  0.101909  0.253416  0.244875 -0.091224   \n",
       "\n",
       "                                             \n",
       "                                             \n",
       "          764       765       766       767  \n",
       "112 -0.086485  0.057876  0.175473 -0.210865  \n",
       "365  0.089164 -0.852266  0.197400 -0.039202  \n",
       "339 -0.279513 -0.523560  0.094154 -0.447300  \n",
       "329 -0.428366  0.086692 -0.039449  0.570365  \n",
       "286 -0.456732  0.276496 -0.105577 -0.123156  \n",
       "..        ...       ...       ...       ...  \n",
       "50   0.287313 -0.191112  0.084220  0.213388  \n",
       "97  -0.020718 -0.154120  0.285610 -0.186035  \n",
       "355 -0.084024 -0.085960 -0.028197 -0.391466  \n",
       "388 -0.282126  0.020250 -0.107082  0.115247  \n",
       "282 -0.127410 -0.465330  0.109324  0.090790  \n",
       "\n",
       "[320 rows x 770 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d629d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ventus/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.99375, 'test': 0.8125}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(\n",
    "\tx_train=train_features_df.features.values,\n",
    "\ty_train=train_features_df.label.values.squeeze(),\n",
    "\tx_test=test_features_df.features.values,\n",
    "\ty_test=test_features_df.label.values.squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff89e6",
   "metadata": {},
   "source": [
    "### Word2Vec Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac81338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Word2Vec:\n",
    "\tnoise = 0.5\n",
    "\tword2vec = KeyedVectors.load(\"private/word2vec_100_3_polish.bin\")\n",
    "\n",
    "\tdef softmax_sample(self, data):\n",
    "\t\t# Extract items and values\n",
    "\t\titems, values = zip(*data)\n",
    "\t\t# Compute softmax probabilities\n",
    "\t\texp_values = np.exp(values)\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values)\n",
    "\t\t# Randomly choose an item based on the probabilities\n",
    "\t\tchosen_item = np.random.choice(items, p=probabilities)\n",
    "\t\treturn chosen_item\n",
    "\n",
    "\tdef __call__(self, row: pd.Series) -> pd.Series:\n",
    "\t\trow.text = \" \".join(\n",
    "\t\t\tself.softmax_sample(self.word2vec.similar_by_word(w))\n",
    "\t\t\tif w in self.word2vec and random.random() < self.noise else w\n",
    "\t\t\tfor w in row.text.split()\n",
    "\t\t)\n",
    "\t\treturn row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b366676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ventus/.local/lib/python3.12/site-packages/gensim/models/keyedvectors.py:849: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "      <td>Jestem 2 dni po zabiegu i naprawde widac popra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>True</td>\n",
       "      <td>Jestem 8 dni po zabiegu on naprawde widac popr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>False</td>\n",
       "      <td>Nieprofesjonalna obsługa w barze ( pozdrowieni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>False</td>\n",
       "      <td>Nieprofesjonalna automatyczny w barze oraz poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>False</td>\n",
       "      <td>Pytania podczas laborki potrafią byc ciężkie, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>False</td>\n",
       "      <td>to sa te trafne zaściankowe diagnozy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>False</td>\n",
       "      <td>Na pewno nikomu nie polecam tego hotelu!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>False</td>\n",
       "      <td>Na szczęście nikomu jednak polecam tego hotelu!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>False</td>\n",
       "      <td>Wydałem 150zl plus dojazd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>False</td>\n",
       "      <td>Wydałem 150zl subskrybent dojazd.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "112   True  Jestem 2 dni po zabiegu i naprawde widac popra...\n",
       "112   True  Jestem 8 dni po zabiegu on naprawde widac popr...\n",
       "365  False  Nieprofesjonalna obsługa w barze ( pozdrowieni...\n",
       "365  False  Nieprofesjonalna automatyczny w barze oraz poz...\n",
       "339  False  Pytania podczas laborki potrafią byc ciężkie, ...\n",
       "..     ...                                                ...\n",
       "355  False              to sa te trafne zaściankowe diagnozy.\n",
       "388  False           Na pewno nikomu nie polecam tego hotelu!\n",
       "388  False    Na szczęście nikomu jednak polecam tego hotelu!\n",
       "282  False                         Wydałem 150zl plus dojazd.\n",
       "282  False                  Wydałem 150zl subskrybent dojazd.\n",
       "\n",
       "[640 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_K1_df = augment_data(train_df, augmentation=Word2Vec(), K=1)\n",
    "augmented_train_K1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64ab7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ventus/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 1.0, 'test': 0.75}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_K1_features_df = extract_features(augmented_train_K1_df)\n",
    "logistic_regression(\n",
    "\tx_train=augmented_train_K1_features_df.features.values,\n",
    "\ty_train=augmented_train_K1_features_df.label.values.squeeze(),\n",
    "\tx_test=test_features_df.features.values,\n",
    "\ty_test=test_features_df.label.values.squeeze(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
