{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(43904, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=43904, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "\n",
    "model_name = \"eryk-mazus/polka-1.1b\"\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question, answer):\n",
    "\treturn f\"Pytanie: {question}\\nOdpowiedź: {answer}\"\n",
    "\n",
    "examples = {\n",
    "  \"Kiedy\": [\n",
    "    query(\"Kiedy odbyła się bitwa pod Grunwaldem?\", \"w 1410\"),\n",
    "    query(\"Kiedy miała miejsce wojna secesyjna?\", \"w 1861\"),\n",
    "    query(\"Kiedy odbyła się rewolucja francuska?\", \"w 1789\")\n",
    "  ],\n",
    "\n",
    "  \"Gdzie\": [\n",
    "    query(\"Gdzie znajduje się Mount Everest?\", \"w Himalajach\"),\n",
    "    query(\"Gdzie leży Warszawa?\", \"w Polsce\"),\n",
    "    query(\"Gdzie znajduje się rzeka Amazonka?\", \"w Ameryce Południowej\")\n",
    "  ],\n",
    "\n",
    "  \"Ile\": [\n",
    "    query(\"Ile planet znajduje się w Układzie Słonecznym?\", \"8\"),\n",
    "    query(\"Ile jest krajów na świecie?\", \"195\"),\n",
    "    query(\"Ile wynosi pierwiastek z czterech?\", \"2\")\n",
    "  ]\n",
    "\n",
    "  # \"Czy\": [\n",
    "  #   query(\"Czy Warszawa jest stolicą Polski?\", \"tak\"),\n",
    "  #   query(\"Czy słońce jest planetą?\", \"nie\"),\n",
    "  #   query(\"Czy Ziemia obraca się wokół Słońca?\", \"tak\"),\n",
    "  # ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, examples=[]):\n",
    "  system_prompt = \"\\n\\n\".join(examples)\n",
    "\n",
    "  prompt = f\"{system_prompt}\\n\\n{query(question, \"\")}\"\n",
    "\n",
    "  model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "  generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=126,\n",
    "    do_sample=True,\n",
    "    penalty_alpha=0.6,\n",
    "    top_k=10\n",
    "  )\n",
    "\n",
    "  output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "  # print (50 * '-', flush=True)\n",
    "  # print (f\"Pytanie: \\\"{question}\\\"\", flush=True)\n",
    "\n",
    "  response = output.removeprefix(prompt).strip().replace(\"\\\\n\", \"\\n\").split(\"\\n\")[0].split(\"\\\"\")[0]\n",
    "  # system_prompt = prompt + response\n",
    "  # print (f\"Odpowiedź: \\\"{response}\", flush=True)\n",
    "  # print(output)\n",
    "  return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sentence_prob(sentence_txt):\n",
    "    input_ids = tokenizer(sentence_txt, return_tensors='pt')['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids)\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], input_ids[:, 1:])\n",
    "        seq_log_probs = torch.sum(log_probs)\n",
    "    return seq_log_probs.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(question):\n",
    "\tprefix = question.split()[0]\n",
    "\tif prefix == \"Czy\":\n",
    "\t\treturn max([\"tak\", \"nie\"], key=lambda a: sentence_prob(query(question, a)))\n",
    "\n",
    "\tif prefix in examples:\n",
    "\t\treturn ask(question, examples=examples[prefix])\n",
    "\t\n",
    "\tkeys = list(examples.keys())\n",
    "\treturn ask(question, examples=[examples[keys[i]][i] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"Ile książek o Królu Maciusiu napisał Janusz Korczak?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "for i, question in enumerate(open(\"task4_questions.txt\").readlines()):\n",
    "\tif not i % 100: print(i)\n",
    "\twith open(\"found_answers.txt\", mode=\"a\") as answers:\n",
    "\t\tanswers.write(test(question.strip()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 87.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!python answer_check_for_task4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
