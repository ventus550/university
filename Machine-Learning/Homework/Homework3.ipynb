{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github.com/marekadamczyk44/ml_uwr/blob/main/Homework/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKY6us_cCg4"
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "**For online delivery before 00:00 7.2.22 to your tutor**\n",
    "\n",
    "**Points: 10**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiZk6ikjhsVL"
   },
   "source": [
    "## Problem 1 [1.5p]\n",
    "\n",
    "Consider AdaBoost.\n",
    "Prove that $\\alpha_t = \\frac{1}{2}\\log\\frac{1-\\epsilon_t}{\\epsilon_t}$, where $\\epsilon_t$ is the error rate is the optimal coefficient for the $t$-th weak learner under the exponential loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB9QqTlKFLH9"
   },
   "source": [
    "## Problem 2 [0.5p]\n",
    "\n",
    "Would it make sense to apply boosting in a regression setting using linear regression as the base learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ptwFU57fNeR"
   },
   "source": [
    "## Problem 3 [0.5p]\n",
    "\n",
    "Suppose you apply AdaBoost using fully grown trees as the base learner. What would happen? What would the final ensemble look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk8vTUfxxs3M"
   },
   "source": [
    "## Problem 4 [Bishop 14.7] [1p]\n",
    "\n",
    "Consider a datasample $x$ with label distribution $p(y|x)$ (thus we assume that the sample may be ambiguous). Assume the boosted ensemble returns for this sample a score $f_x$.\n",
    "\n",
    "The expected loss on this sample is \n",
    "$$\n",
    "\\mathbb{E}_{y|x}\\left[e^{-yf_x}\\right] = \\sum_{y\\in\\pm 1}e^{-yf_x}p(y|x)\n",
    "$$\n",
    "\n",
    "Find the value of the model output $f_x$ which minimizes $\\mathbb{E}_{y|x}\\left[e^{-yf_x}\\right]$ and express it in terms of $p(y=1|x)$ and $p(y=-1|x) = 1 - p(y=1|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVFSEXy_g_fo"
   },
   "source": [
    "## Problem 5 [0.5p]\n",
    "\n",
    "Suppose you work on a binary classification problem and train 3 weak classifiers. You combine their prediction by voting. \n",
    "\n",
    "Can the training error rate of the voting ensemble smaller that the error rate of the individual weak predictors? Can it be larger? Show an example or prove infeasibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 (Bishop) [1.5p]\n",
    "\n",
    "Consider $K$-element mixtures of $D$-dimensional binary vectors. Each component of the mixture uses a different Bernoulli distribution for each dimension of the vector:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "p(z=k) &= \\pi_k \\quad \\text{with } 0 \\leq \\pi_k \\leq 1 \\text{ and } \\sum_k\\pi_k = 1\\\\\n",
    "p(x | z=k) &= \\prod_{d=1}^{D} \\mu_{kd}^{x_d}(1-\\mu_{kd})^{(1-x_d)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $x\\in\\{0,1\\}^D$ is a random vector. The $k$-th mixture component is parameterized by $D$ different probabilities $\\mu_{kd}$ of $x_d$ being 1.\n",
    "\n",
    "Do the following:\n",
    "- Write an expression for the likelihood ($p(x;\\pi,\\mu)$).\n",
    "- Compute the expected value of $x$.\n",
    "- Compute the covariance of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 [1.5p]\n",
    "\n",
    "Let $X\\in \\mathbb{R}^{D\\times N}$ be a data matrix containing $N$ $D$-dimensional points. Furthermore assume $X$ is centered, i.e. \n",
    "$$\n",
    "\\sum_{n=1}^N X_{d,n} = 0 \\quad \\forall d.\n",
    "$$\n",
    "\n",
    "Read about the SVD matrix decomposition (https://en.wikipedia.org/wiki/Singular_value_decomposition). \n",
    "\n",
    "Show:\n",
    "- **P7.1** [0.5p] how the singular vectors of $X$ relate to eigenvectors of $XX^T$\n",
    "- **P7.2** [1p] that PCA can be interpreted as a matrix factorization method, which finds a linear projection data which retains the most information about $X$ (in the least squares sense)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8 [2p]\n",
    "\n",
    "Consider a symmetric real matrix A, i.e. $A\\in \\mathbb{R}^{D\\times D}$ and $A=A^T$.\n",
    "\n",
    "1. Show that the eigenvalues of $A$ are real. Hint: try to multiply by the Hermitean transpose of the corresponding eigenvector.\n",
    "\n",
    "2. Show that if two eigenvalues are different, their eigenvectors are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9 (Bishop) [1p]\n",
    "\n",
    "Recall the SVM training problem\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\min_{w,b} & \\frac{1}{2}w^T w \\\\\n",
    "\\mbox{s.t. } & y_i(w^Tx_i+b) \\geq 1\\qquad \\textrm{for all } i.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Show that the solution for the maximum margin hyperplane doesn't change when the $1$\n",
    "on the right-hand side of the contraints is replaced by any $\\gamma>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Homework3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
