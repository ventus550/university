{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKY6us_cCg4"
   },
   "source": [
    "# Homework 2\n",
    "\n",
    "**For exercises in the week 6-10.12.21**\n",
    "\n",
    "**Points: 10 + 3bp bonus point**\n",
    "\n",
    "Please solve the problems at home and bring to class a [declaration form](http://ii.uni.wroc.pl/~jmi/Dydaktyka/misc/kupony-klasyczne.pdf) to indicate which problems you are willing to present on the backboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q54xMuOWWsX7"
   },
   "source": [
    "## Problem 1 [1p]\n",
    "\n",
    "Consider a binary classification problem with discrete features, which we encode using the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding. E.g. suppose we have 2 features, $x_1\\in{a,b}$ and $x_2\\in{s,t,u}$. Then the encoded data is a real vector $x_e\\in\\mathbb{R}^5$:\n",
    "\n",
    "| $x_1$, $x_2$ | $x_e$ |\n",
    "|:------------:|:-----:|\n",
    "|     a,s      | 10100 |\n",
    "|     b,s      | 01100 |\n",
    "|     ...      |  ...  |\n",
    "|     b,u      | 01001 |\n",
    "\n",
    "Suppose you train a logistic regression classifier and a naive Bayes one.\n",
    "\n",
    "Show that the decision boundary of the two classifiers has the same form, i.e. that you could e.g. use a linear regression implementation to simulate the behavior of a naive Bayes model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ru6gzK-PR8"
   },
   "source": [
    "## Problem 2 (Weighted least squares) [1p + 1bp]\n",
    "\n",
    "Consider a least squares problem in which we apply a (known) weigth $w^{(i)}$ to each example:\n",
    "\n",
    "$$\n",
    "\\min_\\Theta \\frac{1}{2}\\sum_i w^{(i)}(x^{(i)}\\Theta - y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "Write down the closed-form formula for predicting a single value at $x$.\n",
    "\n",
    "The weighted least squares are used in the Local \"[LOESS](https://en.wikipedia.org/wiki/Local_regression)\" regression: we find the set of closest neighbors to a query point, weight them based on the distance to the query, then predict the answer according to the weighted least squares. \n",
    "\n",
    "Describe the LOESS method for the bonus point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYFL1cWQbv1D"
   },
   "source": [
    "## Problem 3 (McKay 4.1) [1p]\n",
    "\n",
    "You are given a set of 12 balls in which:\n",
    "- 11 balls are equal\n",
    "- 1 ball is different (either heavier or lighter).\n",
    "\n",
    "You have a two-pan balance. How many weightings you must use to detect toe odd ball?\n",
    "\n",
    "*Hint:* A weighting can be seen as a random event. You can design them to maximize carry the most information, i.e. to maximize the entropy of their outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00IQ8eslcHVI"
   },
   "source": [
    "## Problem 4 (Murphy, 2.17) [1p]\n",
    "\n",
    "Expected value of the minimum.\n",
    "\n",
    "\n",
    "- Let $X, Y$ be sampled uniformily on the interval $[0,1]$. What is the expected value of $\\min(X,Y)$?\n",
    "- Let $X,Y$ be independent and exponentialy distributed with $X \\sim \\exp(\\lambda_x), Y \\sim \\exp(\\lambda_y)$. What is the expected value of $\\min(X,Y)$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sYUtEqUQWYC"
   },
   "source": [
    "## Problem 5 [1p]\n",
    "\n",
    "Find the gradient with respect to $x$ of\n",
    "$-\\log(S(\\mathbf{x})_j)$, where $S$ is the\n",
    "softmax function (https://en.wikipedia.org/wiki/Softmax_function) and we are\n",
    "interested in the derivative over the $j$-th output of the Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0BwK5qnaxav"
   },
   "source": [
    "## Problem 6 [1p]\n",
    "\n",
    "Consider regresion problem, with $M$ predictors $h_m(x)$ trained to aproximate a target $y$. Define the error to be $\\epsilon_m(x) = h_m(x) - y$.\n",
    "\n",
    "Suppose you train $M$ independent classifiers with average least squares error\n",
    "$$\n",
    "E_{AV} = \\frac{1}{M}\\sum_{m=1}^M \\mathbb{E}_{x}[\\epsilon_m(x)^2].\n",
    "$$\n",
    "\n",
    "Further assume that the errors have zero mean and are uncorrelated:\n",
    "$$\n",
    "\\mathbb{E}_{x}[\\epsilon_m(x)] = 0\\qquad\\text{ and }\\qquad\\mathbb{E}_{x}[\\epsilon_m(x)\\epsilon_l(x)] = 0\\text{ for } m \\neq l\n",
    "$$\n",
    "\n",
    "Let the mean predictor be\n",
    "$$\n",
    "h_M(x) = \\frac{1}{M}\\sum_{m=1}^Mh_m(x).\n",
    "$$\n",
    "\n",
    "What is the average least squares error of $h_M(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ded8rFDBWXBz"
   },
   "source": [
    "## Problem 7: Numerical stability of SoftMax [1p]\n",
    "\n",
    "Many classifiers, such as Naive Bayes, multiply probabilities. To prevent loss of precision due to numerical underflow, we often prefer to add log-probabilities, rather than to multiply probabilities. However, we sometimes also need to add probabilities. This happens e.g. during normalization:\n",
    "- in Naive Bayes, to get final prbabilities we need to comput a sum-exp of the log-scores.\n",
    "- in Softmax, we again compute a sum-exp of the scores.\n",
    "\n",
    "Going back to logarithms reuires us to compute a log-sum-exp operation, or in other words using expoentiation, then addition, and finally re-application of the logarithm.\n",
    "\n",
    "Explain when the log-sum-exp operation may fail numerically, and how this failure can be prevented. Show how this fix can also be applied to the SoftMax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IzGTnZuMxTB"
   },
   "source": [
    "## Problem 8.1 [1p]\n",
    "\n",
    "We have $X_1, X_2, X_3$ random variables such that correlation of each two of them is equal to $\\rho$. Find the smallest value of $\\rho$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB9QqTlKFLH9"
   },
   "source": [
    "## Problem 8.2 [2bp]\n",
    "\n",
    "We have $X_1, X_2, X_3, ..., X_n$ random variables such that correlation of each two of them is equal to $\\rho$. Find the smallest value of $\\rho$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9 [2p]\n",
    "\n",
    "Consider a dataset $X = (x_1,...,x_n),y = (y_1,...,y_n)$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$. We assume that the data generated was according to the following probabilistic model: $y_i = \\Theta \\cdot x_i + \\epsilon_i$ for each $i$, where $\\epsilon_i$ is IID error term distributed according to a zero-mean Laplace distribution, and $\\Theta$ is some unknown but fixed vector. Laplace distribution has PDF of form $x \\mapsto \\frac{1}{2b}e^{-\\frac{|x|}{b}}$. Show that the $\\bar\\Theta$ found by minimizing the Mean Absolute Error value of $\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\Theta x_i|$ is in fact the Maximum Likelihood Estimator of the model. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
