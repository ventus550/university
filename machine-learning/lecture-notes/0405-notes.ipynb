{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsvMSwDHl89p"
   },
   "source": [
    "# Machine Learning @ UWr 2021\n",
    "\n",
    "**Lecture 04 and 05 part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U593Oin0oUaJ"
   },
   "source": [
    "In the previous lecture we have seen a linear regression model which approximates the dataset, and hence makes the predictions, based on the assumption that it's somehow close in it's shape to a line. This model was used for regression problems, i.e., when we wanted to predict numerical values.\n",
    "\n",
    "In this lecture we are going to turn back to classification problems. But as in the linear regression, we are going to assume that the data is (or approximately is) linearly separable.\n",
    "\n",
    "We will also revisit Linear Regression, taking a closer look at its loss function and understanding what tradeoffs we can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmOqB2dUsJYO"
   },
   "source": [
    "## Logistic Regression for Irises\n",
    "\n",
    "Lets get back to our Iris classification problem: preidct the flower species, based on `petal length` and `petal width` features (we limit ourselves to 2 dimesions to be able to plot what is happening).\n",
    "\n",
    "Let's first load the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stPNfJ2CzPZx",
    "outputId": "305e75e0-f04d-4118-e966-f9a95af669f7"
   },
   "outputs": [],
   "source": [
    "# Env setup!\n",
    "# Please note that this code needs only to be run in a fresh runtime.\n",
    "# However, it can be rerun afterwards too.\n",
    "!pip install -q gdown httpimport\n",
    "![ -e cifar.npz ] || gdown 'https://drive.google.com/uc?id=1oBzZdtg2zNTPGhbRy6DQ_wrf5L5OAhNR' -O cifar.npz\n",
    "![ -e mnist.npz ] || gdown 'https://drive.google.com/uc?id=1QPaC3IKB_5tX6yIZgRgkpcqFrfVqPTXU' -O mnist.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "akcwA0SpzV62"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as sstats\n",
    "\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "from matplotlib import animation, pyplot, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import httpimport\n",
    "with httpimport.github_repo('janchorowski', 'nn_assignments', \n",
    "                            module='common', branch='nn18'):\n",
    "     from common.plotting import plot_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "l3Ie2t-wruCk",
    "outputId": "566c71dd-8f39-4c27-8fec-ea4b1ed0aceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Targets:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'petal_width')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(\"Features: \", iris.feature_names)\n",
    "print(\"Targets: \", iris.target_names)\n",
    "petal_length = iris.data[:, iris.feature_names.index(\"petal length (cm)\")].reshape(-1, 1)\n",
    "petal_width = iris.data[:, iris.feature_names.index(\"petal width (cm)\")].reshape(-1, 1)\n",
    "\n",
    "\n",
    "plt.scatter(petal_length, petal_width, c=iris.target, cmap=\"spring\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tt-mfFqNr7mi"
   },
   "source": [
    "We will first simplify our task and consider binary classification: tell Versicilors from Virginicas.\n",
    "\n",
    "Only then, we will see how to generalize our approach to more classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCYRVK840vHb"
   },
   "source": [
    "### Logistic Regressoin intuitions\n",
    "\n",
    "Looking at the Iris scaterplot, we intuitively see that we could draw a line which approximately separates the Versicolors from Virginicas. While it will not correctly classify all flowers near the boundary region, it will do a decent job for the more distant ones.\n",
    "\n",
    "One such guesstimated line may be $6.5 - \\text{petal length} - \\text{petal width}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "8LDE2s5Gr8Rj",
    "outputId": "2d543a1b-fd05-4fb0-ca11-8323db50007b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the petal_length and petal_width of versicolors and virginicas\n",
    "IrisX = np.hstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisX = IrisX[iris.target != 0, :]\n",
    "\n",
    "# Set versicolor=0 and virginia=1\n",
    "IrisY = (iris.target[iris.target != 0] - 1).reshape(-1, 1).astype(np.float64)\n",
    "\n",
    "plt.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY, cmap=\"spring\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")\n",
    "ylim = plt.ylim()\n",
    "\n",
    "# Guesstimate a separation boundary\n",
    "plt.plot(plt.xlim(), 6.5 - np.array(plt.xlim()))\n",
    "plt.ylim(*ylim)\n",
    "_ = plt.text(3,2,\"6.5 - PL - PW = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTNgiOjQ5TEZ"
   },
   "source": [
    "### Why not regress features directly onto probabilities\n",
    "\n",
    "How can we obtain this separating line?\n",
    "\n",
    "One posible solution would be to simply treat the binary class identifier as a numerical value, and fit a linear regression model, hpoing that it will output a number close to 0 for one class and close to 1 for the other class. \n",
    "\n",
    "However, this simplified approach has certain disadvantages. For instance, how do we interpret an output of 2? Is the model extremely confident about its output? \n",
    "\n",
    "Moreover, the least squares criterion doesn't make much sense: for instance, when $y=1$, it penalizes an output of 2 in the same way as an noutput of 0 (the distance from the target 1 is the same in both cases)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzGyWLlsr-cS"
   },
   "source": [
    "### Regression with a squasing function\n",
    "\n",
    "Getting back to classification problem, we want to model the conditional probability \n",
    "$$\n",
    "p(\\text{class}|\\text{petal length}, \\text{petal width}) = p(y|x)\n",
    "$$\n",
    "\n",
    "This conditioanl probability should depend on the distance of a point to the separating boundary. Points near the separating line are ambiguous - probablity there should be close to 0.5. On the other hand, the model should be failry certain on points far away from the separating boundary.\n",
    "\n",
    "We thus need to squash the distance from the boundary to the $0-1$ range of valid probabilities. We will accomplish this by mapping the distance through a \"sigmoid\" (meaning S-shaped function). A very popular function is the logistic sigmoid:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Let's see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "YNz3FIeD6hqr",
    "outputId": "9fe6c936-9607-49c7-8830-54f8e94d4be8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'The logistic sigmoid function')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnEklEQVR4nO3dd3gc5bn+8e+jLtmy5SI3We4NA7YxMr1XAwbyg4QSWjiUlAMhgVBCOCmkkOMQSAgEAgkkEBIOLcSAwTRTEgJxwQXb2Ja7XGVLllWsus/vj10TIWSrrTy7q/tzXbq0OzM788xo99a77zRzd0REJP4lBV2AiIhEhwJdRCRBKNBFRBKEAl1EJEEo0EVEEoQCXUQkQSjQE5iZ/dDM/hylef3RzH4ShflcYmavtfO1S8zshI7W0Gh+r5jZFdGaXzSWa2bDzMzNLGUv48ea2QIzKzezb3ZupZ9Z7hAzqzCz5P21TGm7Zt80Eh/MrKLR0yygBmiIPP/q/q+oZe7+JPBkS9OZ2R+BIne/o9FrD4xyLWdEc377abm3ALPdfVKUymmWma0Frnb3NwDcfT3QvTOXKR2nFnocc/fue36A9cDZjYa1GJoSl4YCS4IuQmKTAj3xpZnZ45Gv6EvMrGDPCDMbZGbPmVmxma1py1d4M7vGzArNrMTMZpjZoEbjTjOz5WZWZma/NbN3zOzqyLivmNk/Io/NzO41s21mtsvMFpvZQWZ2LXAJcEvka/6LkenXmtkpkcfJZna7ma2KrNs8M8tvps4MM/uzme0ws51mNsfM+kfGvd2ormQz+6WZbY9si+sad31Epv2Jmb2/pyYz62NmT0Zqn2Nmwxot96jIsLLI76MajWu63Lsjy10NnLWPbf4WcCJwf6SGMY3n1XT7Rp67mX3NzFZG1v8BM7Mmf8dlkW241Mwmm9kTwBDgxchybmnaFRR578yI/P0LzeyaRvP8oZk9vbf3nXQeBXriOwd4CsgBZgD3A5hZEvAisBDIA04GvmVmp7c0QzM7CbgLuAAYCKyLLAMz6ws8C3wX6AMsB45qfk6cBhwHjAF6Rua3w90fJtwtMz3ybePsZl57I3AxcCbQA/gvoKqZ6a6IzDs/Us/XgN3NTHcNcAYwCZgMfKGZaS4CLiO8vUYC/wIeA3oDy4AfRLZBb+Bl4L7IMu8BXjazPntZ7jTgEKAA+GIz0wDg7icB7wHXRbbLir1N28Q0YAowgfA2Pj1S55eAHwKXE96G5xDe/pfx2W9805uZ51NAETAoUvPPIu+LPZp930nnUqAnvn+4+0x3bwCeACZGhk8Bct39TnevdffVwCOEQ6sllwCPuvt8d68hHN5HRlqoZwJL3P15d68nHGpb9jKfOiAbGAeYuy9z982tXK+rgTvcfbmHLXT3HXtZRh9glLs3uPs8d9/VzHQXAL929yJ3LwV+3sw0j7n7KncvA14BVrn7G5H1fIZwKEO4lb3S3Z9w93p3/yvwCdDcP6YLgF+5+wZ3LyH8jzLafu7uOyP94LMJ/9OC8Dac7u5zItuw0N3XtTSzyDeho4Fb3b3a3RcAvyf8j2GPvb3vpBMp0BNf4zCtAjIiX5uHAoMiX8N3mtlO4HagfyvmOYhwqxwAd68AdhBuuQ4CNjQa54Rbcp/j7m8Rbrk9AGwzs4fNrEcr1ysfWNWK6Z4AZgFPmdkmM5tuZqnNTPeZups83mNro8e7m3m+Z6fhZ7ZPxDrC26el5bYYqO3Q9D2wp87WbsOmBgEl7l7eaFjT9dvb+046kQK969oArHH3nEY/2e5+Ziteu4nwPwQAzKwb4VbwRmAzMLjROGv8vCl3v8/dDwXGE+56uXnPqFbUP7KlQt29zt1/5O7jCXf9TOOzLck9PlM34bBrr89sn4ghhLdPc8vNbzJdW1QSPsJpjwFteO2+tuG+tv8moLeZZTcatrf1k/1Igd51/RsoN7NbzSwzsnPuIDOb0orX/hW40swmmVk68DPgQ3dfS7jv+GAz+0KkRfbf7CVkzGyKmR0eaTFXAtVAKDJ6KzBiHzX8HvixmY22sAnN9VGb2YlmdrCFj5/eRbgLJtR0OuBp4AYzyzOzHODWljbCPswExpjZl80sxcwuJPwP66W9LPebZjbYzHoBt7VxWQuA88wsy8xGAVe14bW/B75jZodGtuEoM9vzj2iv29/dNwDvA3dZeKfzhMhyo3LOg7SfAr2LivRtTiPcn7oG2E74A96zFa99A/gf4DnCLcyRRPre3X078CVgOuFumPHAXMLHyDfVg3C/fSnhr+w7gF9Exv0BGB/pDnqhmdfeQzgMXyMc1H8AMpuZbgDhnbS7CO+4fIdwN0xTj0TmtQj4iHAo1/Of4/pbLdKXPw24KbJOtwDTItumueXOIrxzej7wfBsXdy9QSziA/0QrjvFvVOczwE+BvwDlwAuEd/BCuC//jsj2/04zL78YGEa4tf434Ad7jlmX4JhucCGdKXI0TRFwibvPDrqe1jKzM4CH3L1p14lIzFILXaLOzE43s5xId8ztgAEfBFzWPkW6nc6MdJHkET4E8W9B1yXSFgp06QxHEj56YjvhQ/W+4O7NHfsdSwz4EeHun48Id898P9CKRNpIXS4iIglCLXQRkQQR2IH+ffv29WHDhgW1eBGRuDRv3rzt7p7b3LjAAn3YsGHMnTs3qMWLiMQlM9vr2cTqchERSRAKdBGRBKFAFxFJEAp0EZEEoUAXEUkQLQa6mT1q4VuEfbyX8WZm90VuQ7XIzCZHv0wREWlJa1rofwSm7mP8GcDoyM+1wIMdL0tERNqqxePQ3f1da3Tz22acCzweuTPNB5GLMg1sw63ERERinrtTUx9id20D1fUN1NSFPv1dUx+itj5ETX0DtfUhahvCz+tDTl3kcV2DU98Qoq4hxMkH9Gdifk7Ua4zGiUV5fPYWWkWRYZ8LdAvfzf1agCFD2npjFhGRtnN3KmsbKK2spWx3HTur6ijbHf4pr65jV3UdFdX1lNfUU1FdT1VtA5W19VTVhH/vrm2gqraB3XVtvjT+XvXvmRGzgd5qkbu5PwxQUFCgq4KJSLvVNYTYuquaLWXVbC6rZuuuaooraijeVcP2ylq2l9ewo7KG0so6ahuau0lVWJJB9/QUsjNS6Z6eQrf0ZLqnp9A/O4OstGQy05LDv1OTydjzOzWZjNQk0lOSSU9JIiM1mbSUJNKSk8K/U5JITfrP45RkIy05iZQkIznJCN+ZMfqiEegb+ew9EQejewuKSBSUVNayuriCtTuqWL+jkvUlVRSV7qaodDdby6tperHYtOQkcrPT6ZudzsCeGRw4qAe9u6fROyuNXllp5GSlkpOVRs/MVHpkptAjI5WstOROC9j9LRqBPgO4zsyeAg4HytR/LiJtUVZVx9LNu1i+ZRfLt5azYmsFq4or2FlV9+k0SQYDe2aS3zuTY0b3JS8nk0E5GQzomcmAHhn075FOz8zUhAnn9mgx0M3sr8AJQF8zKyJ8J5dUAHd/iPC9F88ECoEq4MrOKlZE4t/u2gYWbyxj/vpSFhXtZPHGMjaU/Of+JzlZqYzpn82ZBw9kZG53RvTtxtA+WQzulUVaik6d2ZfWHOVycQvjnfCd3UVEPmdXdR1z1pTw4ZoSPly9g4837aIhFO4rGdI7iwl5OVx82BAOHNSTcQOy6Zed3qVb2R0R2OVzRSQxhULO4o1lvL28mPdWFvPRhp00hJy05CQm5efwteNHMHlILybl59Cne3rQ5SYUBbqIdFhdQ4j3V+1g1pItvLlsK1t31WAGB+f15OvHj+ToUX05ZEgOGanJQZea0BToItIu7s6ctaX8fcFGXvl4CyWVtWSlJXPc6FxOHd+fE8f1o3e3tKDL7FIU6CLSJlt3VfPsvCKembuBtTuqyEhN4pQD+nP2xEEcPyZXrfAAKdBFpEV7WuN/en8try7ZQkPIOXx4b64/aTRTDxpAt3RFSSzQX0FE9qoh5MxcvJmH3lnFkk276JmZytXHDOfiw4YwrG+3oMuTJhToIvI5dQ0hnp9fxINvr2LtjipG5nbjrvMO5guT8shMU5dKrFKgi8inQiHnxUWbuPf1FazdUcVBeT146NLJnDZ+AElJOjY81inQRQSA91dt5ycvLWPp5l2MG5DN7y8v4OQD+ukknziiQBfp4opKq/jZzGXMXLyFvJxMfnXhJM6ZOEgt8jikQBfpouobQjzy3hp+9cYKzODGU8dw7XEjdNhhHFOgi3RBSzaVcetzi/h44y5OG9+fH5xzIHk5mUGXJR2kQBfpQuobQjwwexX3vbWSXllpPHjJZM44eGDQZUmUKNBFuogNJVV8+/8WMHddKedOGsSPzjmQnCydmp9IFOgiXcDMxZu59dlFAPzqwkl84ZC8gCuSzqBAF0lg9Q0hps9azsPvrmZSfg6/ufgQ8ntnBV2WdBIFukiC2l5Rw3V/mc8Hq0u47Iih/M+08brjT4JToIskoJVby7nyj3MoLq/hl1+ayPmHDg66JNkPFOgiCeb9wu189c/zSE9J5umvHsnE/JygS5L9RIEukkCen1/ELc8uYkRuNx79yhQG91J/eVeiQBdJEH96fy0/mLGEo0b24aHLDqVHRmrQJcl+pkAXiXPuzgOzC7n7tRWcNr4/9118iE7f76IU6CJxzN2ZPms5D769ivMOyWP6FyeQkqwjWboqBbpIHLvn9RU8+PYqvnz4EH5y7kG6QmIXp3/lInHqvjdX8pu3CrloSr7CXAAFukhc+t07q7jn9RWcP3kwP/t/ByvMBVCgi8SdZ+Zu4K5XPmHahIFM/+IEhbl8SoEuEkdmf7KN255fzDGj+nLPBZNIVphLIwp0kTjx0fpSvvHkfA4YmM1Dlx2q67LI5+gdIRIHNpRUcfWf5pKbnc5jXzmM7uk6QE0+r1WBbmZTzWy5mRWa2W3NjB9iZrPN7CMzW2RmZ0a/VJGuqaKmnmsen0ttQ4jHrpxCbnZ60CVJjGox0M0sGXgAOAMYD1xsZuObTHYH8LS7HwJcBPw22oWKdEWhkPOtpxawclsFv71kMiNzuwddksSw1rTQDwMK3X21u9cCTwHnNpnGgR6Rxz2BTdErUaTr+sVry3lj2Va+P208x47ODbociXGtCfQ8YEOj50WRYY39ELjUzIqAmcD1zc3IzK41s7lmNre4uLgd5Yp0Ha8s3vzpWaCXHzk06HIkDkRrp+jFwB/dfTBwJvCEmX1u3u7+sLsXuHtBbq5aGyJ7s6q4gpufXcSk/Bx+ePaBmOnwRGlZawJ9I5Df6PngyLDGrgKeBnD3fwEZQN9oFCjS1VTV1vP1P88jLSWJ314yWYcnSqu15p0yBxhtZsPNLI3wTs8ZTaZZD5wMYGYHEA509amItJG7c/vzi1m5rYL7LjqEQTmZQZckcaTFQHf3euA6YBawjPDRLEvM7E4zOycy2U3ANWa2EPgr8BV3984qWiRRPTOviBcWbOLGU8ZwzGh9yZW2adXZCe4+k/DOzsbDvt/o8VLg6OiWJtK1rC6u4IczlnDkiD5848RRQZcjcUidcyIxoKa+gW8+9RFpKUnce6Gu0SLto/OHRWLA3bOW8/HGXTx82aEM6JkRdDkSp9RCFwnY+6u288h7a7jk8CGcduCAoMuROKZAFwlQeXUdNz+ziOF9u3HHWU2vqCHSNupyEQnQT19exuay3TzztSPJTEsOuhyJc2qhiwRk9vJtPDVnA9ccN4JDh/YOuhxJAAp0kQCUVdVx23OLGNO/O98+ZUzQ5UiCUJeLSADuemUZxeU1PHJ5ARmp6mqR6FALXWQ/+9eqHeGulmNHMGFwTtDlSAJRoIvsR9V1Ddz+t8UM6Z3Ft9TVIlGmLheR/ei+N1eyZnslT159uI5qkahTC11kP1m2eRe/e3c1FxQM5uhRuvCWRJ8CXWQ/CIWcO174mJ6Zqdx+5gFBlyMJSoEush88O7+IeetK+e4Z48jJSgu6HElQCnSRTlZaWctdM5dRMLQX508eHHQ5ksAU6CKdbPqs5eyqrufHXziIJF0WVzqRAl2kEy3YsJOn5qznyqOGccDAHkGXIwlOgS7SSUIh5wczltC3ezo3nDI66HKkC1Cgi3SSv320kYUbdnLr1HFkZ6QGXY50AQp0kU5QUVPPz1/9hIn5OZx3SF7Q5UgXoTNFRTrB/W8VUlxew8OXHaodobLfqIUuEmXrdlTy6D/WcP7kwRwypFfQ5UgXokAXibK7Zn5CSrJx69SxQZciXYwCXSSK/r2mhFeXbOHrx4+kX4+MoMuRLkaBLhIloZDz05eXMqBHBlcfOyLocqQLUqCLRMmMhZtYWFTGzaeP1aVxJRAKdJEoqK5rYPqrn3BQXg/+nw5TlIAo0EWi4NF/rmFTWTXfO3O8DlOUwCjQRTqotLKWB99excnj+nHkyD5BlyNdmAJdpIMemF1IZU09t0wdF3Qp0sUp0EU6YENJFY//ax3nTx7M2AHZQZcjXVyrAt3MpprZcjMrNLPb9jLNBWa21MyWmNlfolumSGy69/UVmMG3Tx0TdCkiLV/LxcySgQeAU4EiYI6ZzXD3pY2mGQ18Fzja3UvNrF9nFSwSK5Zu2sXfFmzk2uNGMCgnM+hyRFrVQj8MKHT31e5eCzwFnNtkmmuAB9y9FMDdt0W3TJHYM33WJ/TISOUbx48KuhQRoHWBngdsaPS8KDKssTHAGDP7p5l9YGZTm5uRmV1rZnPNbG5xcXH7KhaJAf9eU8Lby4v5+gkj6Zmla51LbIjWTtEUYDRwAnAx8IiZ5TSdyN0fdvcCdy/Izc2N0qJF9i93Z/qrn9AvO50rjhwWdDkin2pNoG8E8hs9HxwZ1lgRMMPd69x9DbCCcMCLJJzZy7cxd10p3zx5tE7xl5jSmkCfA4w2s+FmlgZcBMxoMs0LhFvnmFlfwl0wq6NXpkhsCIWc6a8uZ2ifLC6ckt/yC0T2oxYD3d3rgeuAWcAy4Gl3X2Jmd5rZOZHJZgE7zGwpMBu42d13dFbRIkF5cdEmPtlSzo2njiE1WadxSGwxdw9kwQUFBT537txAli3SHnUNIU695x0yUpOZ+c1jdc0WCYSZzXP3gubGqYkh0krPzSti7Y4qvnPaWIW5xCQFukgr1NQ3cN+bK5mYn8PJB+i8OYlNCnSRVvjrh+vZVFbNzaeNxUytc4lNCnSRFlTV1nP/7FUcMaI3R4/S5XEldinQRVrw+L/Wsb2ihu+odS4xToEusg/l1XU89M4qjh+TS8Gw3kGXI7JPCnSRfXjsn2vZWVXHTafp8rgS+xToIntRVlXHI++t5tTx/ZkwOCfockRapEAX2YtH3ltNeXU9N+rmFRInFOgizdhRUcNj/1zDWRMGcsDAHkGXI9IqCnSRZvzu3dXsrmvg26fooqESPxToIk1sK6/m8X+t5dxJeYzqpxs/S/xQoIs08eDbq6hrcG44Wa1ziS8KdJFGNpft5skP13P+5DyG9e0WdDkibaJAF2nkgdmFuDvXn6TWucQfBbpIRFFpFf83ZwMXFOST3zsr6HJE2kyBLhLxmzcLMYz/PnFU0KWItIsCXQRYu72SZ+cX8eXDhzAoJzPockTaRYEuAtz35kpSkoxvnDAy6FJE2k2BLl1e4bZyXliwkSuOGka/HhlBlyPSbgp06fLufWMlGanJfPW4EUGXItIhCnTp0pZt3sXLizZz5dHD6NM9PehyRDpEgS5d2r2vryA7PYVrjlXrXOKfAl26rEVFO3lt6VauOnY4OVlpQZcj0mEKdOmy7n5tBb2yUrnqmOFBlyISFQp06ZL+vaaEd1cU87XjR5KdkRp0OSJRoUCXLsfduXvWcnKz07n8yGFBlyMSNQp06XLeW7mdf68t4fqTRpGZlhx0OSJRo0CXLsXd+eVry8nLyeTCKflBlyMSVQp06VJmLdnKwqIybjh5NOkpap1LYlGgS5fREHLufm05I3O7cd7kvKDLEYm6VgW6mU01s+VmVmhmt+1juvPNzM2sIHolikTH8/OLKNxWwXdOG0tKstoyknhafFebWTLwAHAGMB642MzGNzNdNnAD8GG0ixTpqJr6Bn71xkomDO7J1IMGBF2OSKdoTTPlMKDQ3Ve7ey3wFHBuM9P9GPhfoDqK9YlExV8+XM/Gnbu5+fSxmFnQ5Yh0itYEeh6wodHzosiwT5nZZCDf3V/e14zM7Fozm2tmc4uLi9tcrEh7VNTU88DsQo4a2YdjRvUNuhyRTtPhjkQzSwLuAW5qaVp3f9jdC9y9IDc3t6OLFmmVR95dzfaKWm6ZOk6tc0lorQn0jUDjA3YHR4btkQ0cBLxtZmuBI4AZ2jEqsaC4vIZH3lvNWQcPZFJ+TtDliHSq1gT6HGC0mQ03szTgImDGnpHuXubufd19mLsPAz4AznH3uZ1SsUgb3PfmSmrrQ3zn9LFBlyLS6VoMdHevB64DZgHLgKfdfYmZ3Wlm53R2gSLttWZ7JX/993ouPmwIw/t2C7ockU6X0pqJ3H0mMLPJsO/vZdoTOl6WSMfdPWs5aSlJfPPk0UGXIrJf6OwKSUgfrS/l5cWbufrYEeRm69Zy0jUo0CXhuDs/eXkZudnpuvGzdCkKdEk4MxdvYd66Um46dQzd0lvVqyiSEBToklBq6hv4+avLGDcgmy8V6PK40rUo0CWhPP7+OjaU7Ob2Mw8gOUknEUnXokCXhFFaWctv3lrJ8WNyOW6MzkSWrkeBLgnjl68vp7K2ge+ddUDQpYgEQoEuCWHppl385cP1XHbEUMb0zw66HJFAKNAl7rk7P3pxCT0zU/n2KWOCLkckMAp0iXszF2/hwzUl3HjaWHpmpQZdjkhgFOgS16rrGvjZzPBhil8+bEjQ5YgESmddSFz77exCNu7czV+vOUKHKUqXpxa6xK3VxRU89M5qvjBpEEeO7BN0OSKBU6BLXHJ3fjBjCekpSdyuwxRFAAW6xKmZi7fw3srt3HTaGPplZwRdjkhMUKBL3KmoqefOl5Zw4KAeXHrE0KDLEYkZ2ikqcefuWcvZVl7Dg5ceSkqy2iQie+jTIHFl/vpS/vSvtVx+xFAmD+kVdDkiMUWBLnGjtj7Ebc8tYkCPDG6eOi7ockRijrpcJG48+PYqVmyt4A9XFNBdN64Q+Ry10CUurNxazgOzCzl74iBOPqB/0OWIxCQFusS8+oYQ33lmIVnpyXx/2vigyxGJWfreKjHvoXdWsbCojPu/fAi52elBlyMSs9RCl5i2dNMufv3mSqZNGMi0CYOCLkckpinQJWbV1oe48ekF9MxM48fnHhR0OSIxT10uErPufWMFn2wp55HLC+jVLS3ockRinlroEpPeL9zOQ++s4qIp+Zw6Xke1iLSGAl1iTkllLd9+egEj+nbj+2frqBaR1lKXi8QUd+eWZxdRWlnHo1+ZQlaa3qIiraUWusSUJz5YxxvLtnLrGeM4cFDPoMsRiSsKdIkZCzbs5McvLeWkcf34r6OHBV2OSNxpVaCb2VQzW25mhWZ2WzPjbzSzpWa2yMzeNDNdpFrapKSylm/8eR79e2RwzwUTMdP9QUXaqsVAN7Nk4AHgDGA8cLGZNd1T9RFQ4O4TgGeB6dEuVBJXQ8i54amP2F5Zy0OXHkpOlg5RFGmP1rTQDwMK3X21u9cCTwHnNp7A3We7e1Xk6QfA4OiWKYns3tdX8N7K7fz43AM5KE/95iLt1ZpAzwM2NHpeFBm2N1cBrzQ3wsyuNbO5Zja3uLi49VVKwvr7go3cP7uQi6bkc+GUIUGXIxLXorpT1MwuBQqAXzQ33t0fdvcCdy/Izc2N5qIlDi3YsJObn13EYcN7c6dO7RfpsNYc5LsRyG/0fHBk2GeY2SnA94Dj3b0mOuVJotpctptrHp9Lv+x0Hrr0UNJSdMCVSEe15lM0BxhtZsPNLA24CJjReAIzOwT4HXCOu2+LfpmSSMqr67jqj3OpqqnnD1dMobeu0yISFS0GurvXA9cBs4BlwNPuvsTM7jSzcyKT/QLoDjxjZgvMbMZeZiddXE19A199Yh4rtpbzwCWTGTsgO+iSRBJGq86rdveZwMwmw77f6PEpUa5LElAo5Nz09ELeX7WDey6YyAlj+wVdkkhCUcel7Bfuzp0vLeWlRZv57hnjOG+yjmwViTYFunQ6d+fnr37CH99fy1XHDOfa40YEXZJIQlKgS6e79/UV/O6d1Vx6xBDuOOsAndYv0kkU6NKp7ntzJfe9VciFBfncec5BCnORTqSLTUuncHf+99XlPPTOKs6bnMdd5x1MUpLCXKQzKdAl6kIh53/+/jFPfrieSw4fwo/PPUhhLrIfKNAlqmrqG7jl2UX8fcEmvnb8SG6dOlbdLCL7iQJdomZnVS1ffWIeH64p4ZapY/nGCaOCLkmkS1GgS1Ss21HJlY/Noah0N7++aBLnTtrXBTlFpDMo0KXD/lm4nev+Mh8HnrzmcKYM6x10SSJdkgJd2s3defCdVdw9azkjc7vz8OUFDO/bLeiyRLosBbq0S1lVHbc8t5BZS7Zy1oSBTD9/At3S9XYSCZI+gdJmH6zewY3/t4Bt5TXccdYBXHXMcB3JIhIDFOjSarX1IX795gp++/YqhvbO4rmvH8XE/JygyxKRCAW6tMpH60u59blFrNhawQUFg/nB2Qeqi0UkxugTKftUUVPPva+v4NF/rmFAjwwe/UoBJ43rH3RZItIMBbo0y915YcFG7pr5CdvKa7j0iCHcOnUc2RmpQZcmInuhQJfPmbeuhJ++vIz563cycXBPfnfZoRwypFfQZYlICxTo8qllm3dx96zlvPnJNvp2T2f6FyfwxcmDdWEtkTihQBc+3ljGb98u5JWPt9A9PYWbTx/LlUcPIytNbw+ReKJPbBfl7ry/agcPv7uad1YUk52ewjdOGMk1x44gJyst6PJEpB0U6F1MZU09f/toI396fy0rt1XQp1saN58+lsuOHEoP7fAUiWsK9C7A3Zm3rpSn527gpUWbqapt4OC8ntz9pYlMmzCQjNTkoEsUkShQoCewT7bs4sWFm3hx4WbWl1TRLS2ZsycM4oIp+UwekqPT9UUSjAI9gYRCzkcbSnlt6VZeX7qV1cWVJBkcPaov1580ijMPHqizO0USmD7dcW5LWTXvrSzm3ZXb+cfKYkqr6khJMo4Y0YcrjxrGGQcPpG/39KDLFJH9QIEeR0IhZ82OSuavK2XO2hI+XFPCuh1VAPTtns6JY/tx/NhcThjbj56Z2sEp0tUo0GNUKORs3LmbJZvKWLyxjMUbd7Fww07KdtcB0DMzlcOG9+ayI4Zy1Mi+HDAwW33iIl2cAj1g9Q0hNu7czeriSlYVV1C4rYLlW8tZsaWcytoGAJKTjNH9unPGQQOYPKQXhwzJYWRud53BKSKfoUDvZLX1IbbuqmZzWTWby3ZTVLrnp4r1JVVsLN1Nfcg/nb53tzRG9+vOFw8dzNgBPRg/qAfjBmTr0EIRaVGrAt3MpgK/BpKB37v7z5uMTwceBw4FdgAXuvva6JYaG+oaQpTtrmNnVR1lu2spqayjtKqWkspadlTUsKOiluKKGorLa9hWXkNJZe3n5tG7Wxp5OZkcnNeTaRMGMrR3N0b268aIvt3p1U1naYpI+7QY6GaWDDwAnAoUAXPMbIa7L2002VVAqbuPMrOLgP8FLuyMglvi7tQ1OLUNIWrrQ9TUN1BbH6K6LkR1XUP4p/4/j3fXNlBV28DuugYqa+qpqg3/rmj0U15dT3l1HeXV4fF7k5GaRN/u6fTpnk5+7ywOHdqL3Ox0BvXMZEDPDAb2zCCvV6aukSIinaI1yXIYUOjuqwHM7CngXKBxoJ8L/DDy+FngfjMzd3ei7Ok5G/jdu6uoa3DqG0LUNjj1oRB19aFPg7y9kpOMbmnJZKWlkJ2RQrf08O8BPTLokZFKdkYKPTJTyclKpWdm+Kd3tzR6ZaXRq1sa3dKStWNSRALTmkDPAzY0el4EHL63ady93szKgD7A9sYTmdm1wLUAQ4YMaVfBvbqlMW5gD1KTjJTkJFKTjbTkJFKTk0hJTiItJYm0ZCMtJYn0lGTSU8LDMlLDj9NTkslMC//OSE0mKy38k5mWTFpykgJZROLWfv3u7+4PAw8DFBQUtKv1fur4/pw6XrdAExFpKqkV02wE8hs9HxwZ1uw0ZpYC9CS8c1RERPaT1gT6HGC0mQ03szTgImBGk2lmAFdEHn8ReKsz+s9FRGTvWuxyifSJXwfMInzY4qPuvsTM7gTmuvsM4A/AE2ZWCJQQDn0REdmPWtWH7u4zgZlNhn2/0eNq4EvRLU1ERNqiNV0uIiISBxToIiIJQoEuIpIgFOgiIgnCgjq60MyKgXXtfHlfmpyFGse0LrEnUdYDtC6xqiPrMtTdc5sbEVigd4SZzXX3gqDriAatS+xJlPUArUus6qx1UZeLiEiCUKCLiCSIeA30h4MuIIq0LrEnUdYDtC6xqlPWJS770EVE5PPitYUuIiJNKNBFRBJEXAe6mV1vZp+Y2RIzmx50PR1lZjeZmZtZ36BraQ8z+0Xk77HIzP5mZjlB19RWZjbVzJabWaGZ3RZ0Pe1lZvlmNtvMlkY+HzcEXVNHmFmymX1kZi8FXUtHmFmOmT0b+ZwsM7Mjozn/uA10MzuR8L1MJ7r7gcDdAZfUIWaWD5wGrA+6lg54HTjI3ScAK4DvBlxPmzS6IfoZwHjgYjMbH2xV7VYP3OTu44EjgP+O43UBuAFYFnQRUfBr4FV3HwdMJMrrFLeBDnwd+Lm71wC4+7aA6+moe4FbgLjdS+3ur7l7feTpB4TvbhVPPr0hurvXAntuiB533H2zu8+PPC4nHBx5wVbVPmY2GDgL+H3QtXSEmfUEjiN8/wjcvdbdd0ZzGfEc6GOAY83sQzN7x8ymBF1Qe5nZucBGd18YdC1R9F/AK0EX0UbN3RA9LkOwMTMbBhwCfBhwKe31K8KNnVDAdXTUcKAYeCzSffR7M+sWzQXs15tEt5WZvQEMaGbU9wjX3pvw18kpwNNmNiJWb33XwrrcTri7Jebtaz3c/e+Rab5H+Cv/k/uzNvk8M+sOPAd8y913BV1PW5nZNGCbu88zsxMCLqejUoDJwPXu/qGZ/Rq4DfifaC4gZrn7KXsbZ2ZfB56PBPi/zSxE+II3xfurvrbY27qY2cGE/3MvNDMId1PMN7PD3H3LfiyxVfb1NwEws68A04CTY/Wf6z605oboccPMUgmH+ZPu/nzQ9bTT0cA5ZnYmkAH0MLM/u/ulAdfVHkVAkbvv+ab0LOFAj5p47nJ5ATgRwMzGAGnE4ZXY3H2xu/dz92HuPozwH31yLIZ5S8xsKuGvxue4e1XQ9bRDa26IHhcs3Dr4A7DM3e8Jup72cvfvuvvgyGfjIsI3oI/HMCfymd5gZmMjg04GlkZzGTHdQm/Bo8CjZvYxUAtcEYctwkRzP5AOvB75tvGBu38t2JJab283RA+4rPY6GrgMWGxmCyLDbo/cH1iCcz3wZKTBsBq4Mpoz16n/IiIJIp67XEREpBEFuohIglCgi4gkCAW6iEiCUKCLiCQIBbqISIJQoIuIJIj/D2rrxf1A2QTzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, 1/(1+np.exp(-z)))\n",
    "plt.title('The logistic sigmoid function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YW4aO3o45ec"
   },
   "source": [
    "### Properties of the logistic sigmoid\n",
    "\n",
    "The logistic sigmoif has a few nice properties:\n",
    "\n",
    "1. $\\sigma(0) = 0.5$. This means that points exaclt on the separating line are ambigous, just like we desired.\n",
    "2. $\\lim_{z\\rightarrow \\infty} \\sigma(z) = 1$ and $\\lim_{z\\rightarrow -\\infty} \\sigma(z) = 0$: the further we are from the separating boundary, the more confident the model.\n",
    "3. $\\sigma(-z) = 1 - \\sigma(z)$. The function is symmetrical, it doesn't matter which class we treat as the \"positive\" one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W_78Jxd8JvQ"
   },
   "source": [
    "### Definition of the Logistic Regression model\n",
    "\n",
    "Armed with the $\\sigma$ squashing function, let's define:\n",
    "\n",
    "$$\n",
    "p(y=1|x;\\Theta) = \\sigma(x\\Theta),\n",
    "$$\n",
    "where $x$ is a row vector of features $y$ is a random variable denoting the target class and $\\Theta$ is a column vector of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxdJ5QY191PP"
   },
   "source": [
    "### Logistic regression loss function\n",
    "\n",
    "To fit parameters $\\Theta$, \n",
    "we will again use the Maximum Likelihood principle. Please observe that:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=y^{(i)}|x^{(i)};\\Theta) &= \\cases{p(y=1|x;\\Theta) &if $y^{(i)}=1$ \\\\ 1-p(y=1|x^{(i)};\\Theta) &if $y^{(i)}=0$} \\\\\n",
    "&= p(y=1|x^{(i)};\\Theta)^{y^{(i)}}(1-p(y=1|x^{(i)};\\Theta))^{(1-y^{(i)})} \\\\\n",
    "&= \\sigma(x^{(i)}\\Theta)^{y^{(i)}}(1-\\sigma(x^{(i)}\\Theta))^{(1-y^{(i)})}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore the negative log likelihood ($nll$) is:\n",
    "$$\n",
    "\\begin{split}\n",
    "nll(\\Theta) &= -\\sum_{i=1}^{N} y^{(i)} \\log \\sigma(x^{(i)}\\Theta) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)}\\Theta)) = \\\\\n",
    "&= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This loss function is often referred to as the *cross-entropy* loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suFrszWl8pfz"
   },
   "source": [
    "## Logistic regression training\n",
    "\n",
    "We now need to find the $\\Theta$ which minimizes the likelihood function. Again, we will use calculus and compute the gradient of the nll. We will do it in steps. First please observe how simple is the gradient of the logistic sigmoid function:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial z}\\sigma(z) = \\sigma(z)(1-\\sigma(z))$$\n",
    "\n",
    "Now let $z^{(i)} = x^{(i)}\\Theta$. First lets compute the gradient of the loss on a single sample $nll^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial nll^{(i)}(\\Theta)}{\\partial \\Theta} = \\frac{\\partial nll^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial\\Theta}\n",
    "$$\n",
    "\n",
    "The derivative of the first term on the right hand turns out to be very simple:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial nll^{(i)}}{\\partial z^{(i)}} &= \\frac{\\partial -\\left(y^{(i)} \\log \\sigma(z^{(i)}) + (1-y^{(i)})\\log(1-\\sigma(z^{(i)}))\\right)}{\\partial z^{(i)}} \\\\\n",
    "&= -y^{(i)}\\frac{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}{\\sigma(z^{(i)})} + \n",
    "(1-y^{(i)})\\frac{\\sigma(z^{(i)})(1-\\sigma(z^{(i)}))}{1-\\sigma(z^{(i)})} \\\\\n",
    "&= -y^{(i)}(1-\\sigma(z^{(i)})) + (1-y^{(i)})\\sigma(z^{(i)})\\\\\n",
    "&=\\sigma(z^{(i)})-y^{(i)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The second term $\\frac{\\partial z^{(i)}}{\\partial\\Theta}$ is also easy:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z^{(i)}}{\\partial\\Theta} = \\frac{\\partial x^{(i)}\\Theta}{\\partial\\Theta} = (x^{(i)})^T\n",
    "$$\n",
    "\n",
    "Taken together the loss derivative is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial nll(\\Theta)}{\\partial \\Theta} = \\sum_{i=1}^{N}\\frac{\\partial nll^{(i)}}{\\partial z^{(i)}}\\frac{\\partial z^{(i)}}{\\partial\\Theta} = \\sum_{i=1}^{N}(x^{(i)})^T\\left(\\sigma(z^{(i)})-y^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "The expression can be further simplified by using the data matrix $X$ (of shape $N\\times D$ whose $i$-th row is the $i$-th sample), the target matrix $Y$, letting $Z=X\\Theta$, and assuming that the function $\\sigma()$ is applied separately to all elements of its input.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial nll(\\Theta)}{\\partial \\Theta} = X^T\\left(\\sigma(Z)-Y\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUyMiaG-BVkc"
   },
   "source": [
    "#### Comparison of gradients of Linear and Logistic Regression\n",
    "\n",
    "Recall from Lecture 2, that the linear regression gradient we have derived was proportional to\n",
    "$$\n",
    "X^T(X\\Theta - Y) = X^T E\n",
    "$$\n",
    "where just like for logistic regression $X$ is the $X\\times D$ data matrix, $Y$ is the $X\\times 1$ coliumn vector of targets, and $E$ is the column vector of errors.\n",
    "\n",
    "Notice, that for Logistic regression the gradient takes essentially the same form:\n",
    "\n",
    "$$\n",
    "X^T\\left(\\sigma(X\\Theta) - Y\\right) = X^T E'\n",
    "$$\n",
    "\n",
    "where $E'$ is the \"error\" of logistic regression - the difference betweenn the desired target, encoed as $0$ or $1$ and the probability assigned by the model.\n",
    "\n",
    "This is not a coincidence. Linear and Logistic regresison belong to the family of [generalized linear models](https://en.wikipedia.org/wiki/Generalized_linear_model), which contains many more regression-like models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fL0TNQVBCgS"
   },
   "source": [
    "Unfortunately, the gradient of the loss is no longer linear with $\\Theta$. No closed-form solution exists. Therefore we will search for the minimum of the loss function by taking small steps along the gradient of the loss. We will call this the **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h94G8mlL8pf0"
   },
   "source": [
    "### The Gradient Descent algorithm\n",
    "\n",
    "Due to the nonlinear $\\sigma$ function appearing in the gradient of neg-log likelihood for logistic regression, we can't derive a closed form formula for the zero-crossing of the gradient.\n",
    "\n",
    "Instead we will follow an iterative minimization procedure. Recall that the negated gradient of a function points in the direction of the maximal function decrease (in fact, one can define the gradient to be direction of maximum function increase for an inifitesimally small step).\n",
    "\n",
    "Our approach to solving the linear regression problem will follow this intuition: we will evaluate the neg-log likelihood, take its gradient, then make a small step against the gradient, hopefully getting a smaller neg-log likelihood value. After many such steps we hope to get to a local minimum - a point where the gradient is zero, and we can't go any lower.\n",
    "\n",
    "It turns out, that again our optimization problem is convex, and the local minimum reached with gradient descent will be the global one.\n",
    "\n",
    "The gradient descent algorithm is very simple:\n",
    "\n",
    "* $\\Theta \\gets $ a sane initial value\n",
    "* `While` not converged:\n",
    "    * \n",
    "$\\Theta \\gets \\Theta - \\alpha \\frac{\\partial L(\\Theta)}{\\partial\\Theta}$\n",
    "\n",
    "When GD converges, gradient is close to zero and neg-log likelihood stops to change. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ghi6LnEcPtlI"
   },
   "source": [
    "#### We will now see a demonstration of GD solving the logistic regression on Irises\n",
    "\n",
    "Please note, that unlike previous lectures, the details of the implementation are missing: you will implement logistic regression during the programming assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JzffJYenP6Xn"
   },
   "outputs": [],
   "source": [
    "#@title Logistic regression implementation\n",
    "# This code is part of homework assignment and is removed on purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o40GmInm8pf5",
    "outputId": "647c89e9-8a0c-46cf-da30-beb32a595a8e",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c0c18272838c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#note: the step-size needs to be tuned!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mThetaOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIrisX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIrisY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_tolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Gradient descent made {len(history)} steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Theta is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mThetaOpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg' is not defined"
     ]
    }
   ],
   "source": [
    "#note: the step-size needs to be tuned!\n",
    "ThetaOpt, history = logreg(IrisX, IrisY, alpha=1e-2, stop_tolerance=1e-8, max_steps=100000)\n",
    "print(f\"Gradient descent made {len(history)} steps\")\n",
    "print(\"Theta is: \", ThetaOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "ed1rLNHkSGy4",
    "outputId": "75b0e2f0-dc2b-4bb6-90c7-c4d579813155"
   },
   "outputs": [],
   "source": [
    "# Make an animation of training\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY, cmap=\"spring\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")\n",
    "\n",
    "xlim = np.array(plt.xlim())\n",
    "ylim = plt.ylim()\n",
    "\n",
    "line, = plt.plot([],[])\n",
    "\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "def animate(i):\n",
    "    epoch = 1 + int(1.0 * (len(history)-1) / num_frames * i)\n",
    "    Theta = history[epoch]\n",
    "    ax.set_title('Epoch %d, Theta: %s' % (epoch, Theta,))\n",
    "    yy = (Theta[0] + xlim * Theta[1]) / - Theta[2]\n",
    "    line.set_data(xlim, yy)\n",
    "    return line,\n",
    "\n",
    "num_frames=100\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=30, blit=True)\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBQi2ayAU29A"
   },
   "source": [
    "The animation shows how logistic regression iteratively moves the separating line towards its final location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpxJj_XK8pgT"
   },
   "source": [
    "## Softmax Regression - dealing with multiple classes\n",
    "\n",
    "We will now generalize Linear Regression to handle many classes.\n",
    "\n",
    "The model will produce a probability distribution over the $k$ classes: it will have $k$ outputs, each corresponding to the probability of one class. The outputs will be constrained ot sume to one.\n",
    "\n",
    "Intuitively, the model will first compute a score for each class. The scores are independent, each score is computed using a linear projection of the features:\n",
    "\n",
    "$$\n",
    "z_k = x\\Theta_k,\n",
    "$$\n",
    "\n",
    "where $z_k$ is the score for class $k$ and $\\Theta_k$ is the **vector** of scoring parameters for the $k$-th class.\n",
    "\n",
    "We can egain use matrix algebra and drop the indices:\n",
    "$$\n",
    "z = x\\Theta\n",
    "$$\n",
    "with $z\\in\\mathbb{R}^{1\\times k}$ is the vector of scores, $x\\in 1\\times D$ is the vector with a sample's fetures and $\\Theta\\in{D\\times k}$ is the **matrix** of all paramaters.\n",
    "\n",
    "We will normalize the sores using the SoftMax function:\n",
    "\n",
    "$$\n",
    "\\text{SoftMax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n",
    "$$\n",
    "\n",
    "Please note, that SoftMax works on all the scores at once, it is not an elementwise function. In particular, the output for one class depends on all other ones, beacuse they are jointly normalized to sum to 1.\n",
    "\n",
    "Thus SoftMax regression learns the function\n",
    "\n",
    "$$\n",
    "h(x, \\Theta) = \\text{SoftMax}(x\\Theta).\n",
    "$$\n",
    "\n",
    "And we interpret the model's outputs as probabilities:\n",
    "\n",
    "$$\n",
    "p(y=k|x;\\Theta) = h(x, \\Theta)_k = \\text{SoftMax}(x\\Theta)_k.\n",
    "$$\n",
    "\n",
    "Application of the MLE principle yields the cross-entropy loss, which is analogous to the one for linear regression:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = -\\sum_{i=1}^N\\sum_k[y^{(i)}=k]\\log \\left(h(x^{(i)}, \\Theta)_k\\right)\n",
    "$$\n",
    "\n",
    "where $[y^{(i)}=k]$ is an indicator function which takes value 1 if $y^{(i)}=k$ and 0 otherwise.\n",
    "\n",
    "SoftMax regression is a generalization of logistic regression, which we will show in a homework assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iS7jnT5dupp"
   },
   "source": [
    "Let's now apply SoftMax regression to the full Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KK92WH8Hd2nL"
   },
   "outputs": [],
   "source": [
    "IrisX = np.hstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisY = iris.target.reshape(-1, 1)\n",
    "\n",
    "ThetaIris = softmax_regression(\n",
    "    IrisX, IrisY, 3, maxiter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyvC0jfFecbU",
    "outputId": "1a231d03-19c1-4a3a-8806-8ef8349c5757"
   },
   "outputs": [],
   "source": [
    "IrisPreds = IrisX @ ThetaIris\n",
    "iris_acc = np.mean(IrisPreds.argmax(-1) == IrisY[:,0]) * 100.0\n",
    "print(f\"SoftMax regression training accuracy on Iris: {iris_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyHTkQmE8pgU"
   },
   "source": [
    "### More examples of Softmax Regression\n",
    "\n",
    "Picturing the weigths of Softmax Regression weigths on Iris is challenging, because the weigths only make sense only when taken jointly - for the model to predict a class, it must score high relatively to the other classes.\n",
    "\n",
    "However, on image data the weigths can be pictured as images - it will show us which pixels affect the probabiity of each class.\n",
    "\n",
    "We will use two datasets containing images - MNIST and CIFAR10. They both contain samples belonging to 10 classes. Therefore for each dataset we will find a set of parameters for each of the 10 classes. Then we will display the parameters as images. We can think about them as prototype vectors sensitive to elements of a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXrHhOaq8pgW"
   },
   "outputs": [],
   "source": [
    "with np.load('mnist.npz') as data:\n",
    "    mnist_train_data = data[\n",
    "        'train_data'].astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "    mnist_train_labels = data['train_labels']\n",
    "    mnist_test_data = data[\n",
    "        'test_data'].astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "    mnist_test_labels = data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-NTUTsv8pgc"
   },
   "outputs": [],
   "source": [
    "# note: we want to keep the number of iterations low, afterwards we may \n",
    "# start to overfit. This will be further explained in later lectures.\n",
    "ThetaMnist = softmax_regression(\n",
    "    mnist_train_data, mnist_train_labels, 10, maxiter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hHEhrAI8pgf",
    "outputId": "bbc34ccb-7c12-4f35-8df5-b3787392867e"
   },
   "outputs": [],
   "source": [
    "predicted_train_labels = (mnist_train_data @ ThetaMnist).argmax(1)\n",
    "print (f\"Train error rate: {(predicted_train_labels != mnist_train_labels).mean()*100:.2f}%\")\n",
    "\n",
    "predicted_test_labels = (mnist_test_data @ ThetaMnist).argmax(1)\n",
    "print (f\"Test error rate: {(predicted_test_labels != mnist_test_labels).mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "1WGeSFkZ8pgk",
    "outputId": "590159a7-0eba-4ca4-a4bf-7d42bb0d3841"
   },
   "outputs": [],
   "source": [
    "plot_mat(ThetaMnist[:,:].reshape(28,28,1,-1).transpose(3,2,0,1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxxYVQW98pgp"
   },
   "outputs": [],
   "source": [
    "with np.load('cifar.npz') as data:\n",
    "    cifar_train_data = data[\n",
    "        'train_data'].astype(np.float32).reshape(-1, 32*32*3) / 255.0\n",
    "    cifar_train_labels = data['train_labels']\n",
    "    cifar_test_data = data[\n",
    "        'test_data'].astype(np.float32).reshape(-1, 32*32*3) / 255.0\n",
    "    cifar_test_labels = data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for xx in range(10*5):\n",
    "    plt.subplot(5,10,xx+1)\n",
    "    plt.imshow(cifar_train_data[xx, :].reshape(32,32,3))\n",
    "    plt.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqBn8dff8pgt"
   },
   "outputs": [],
   "source": [
    "# note: we want to keep the number of iterations low, afterwards we may \n",
    "# start to overfit. This will befurther explained in later lectures\n",
    "ThetaCifar = softmax_regression(\n",
    "    cifar_train_data, cifar_train_labels, 10, maxiter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mkNsS9i8pgv"
   },
   "outputs": [],
   "source": [
    "predicted_train_labels = (cifar_train_data @ ThetaCifar).argmax(1)\n",
    "print(f\"Train error rate: {(predicted_train_labels != cifar_train_labels).mean()*100:.2f}%\")\n",
    "\n",
    "predicted_test_labels = (cifar_test_data @ ThetaCifar).argmax(1)\n",
    "print(f\"Test error rate: {(predicted_test_labels != cifar_test_labels).mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkjgZgE68pgz"
   },
   "outputs": [],
   "source": [
    "# We skip the first row of parameters because they represent the \n",
    "# constant term. The remaming ones are rehsaped as RGB images\n",
    "#\n",
    "# The 10 classes are:\n",
    "# ------------------------------------------------\n",
    "# | airplane | automobile | bird  | cat  | deer  |\n",
    "# |----------|------------|-------|------|-------|\n",
    "# |   dog    |     frog   | horse | ship | truck |\n",
    "# ------------------------------------------------\n",
    "#\n",
    "# Again, the learned parameters ressebmle the average image in each class.\n",
    "#\n",
    "plot_mat(ThetaCifar[:,:].reshape(32,32,3,-1).transpose(3,2,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8suMcXja9Peg"
   },
   "source": [
    "## Linear regression revisited\n",
    "\n",
    "We now will explore what consequences on model behavior have the assumptions we can make about the error probability distribution.\n",
    "\n",
    "Our analysis will focus on robustness of some loss functions, understood as their resilience to:\n",
    "- mismatch between our assumptions and the actil data,\n",
    "- errors in the data, most notably sensitivity to erroneous or outlying data points.\n",
    "\n",
    "Before we tackle linear regression, let's consider two estimators for the \"typical value\" of a data sample, namely:\n",
    "- mean,\n",
    "- median.\n",
    "\n",
    "**Observation 1**: tiny movements to any data point change the mean. Moreover, by moving any point by a large amount, we arbitralily move the mean.\n",
    "\n",
    "These properties of the mean result in a fragility of the arithmetic mean. Potentially a large error in a _single_ data point will drastically influence the estimator!\n",
    "\n",
    "On the other hand, if there are no outliers, and the points are really normally distributed, the population mean very quickly converges to the distributions mean. Intuitiely, fast convergence is due to its dependence on all points - it extracts a little bit of information from each data point.\n",
    "\n",
    "**Observation 2**: tiny movements to most data points don't change the median (Intuition: the median lies between two dat points closest to it. If we wiggle one of them a bit, the median will wiggle as well). A large movement to any point cannot arbitraly change the median. Again, the only two points that can influence the median are thw two closest to it. If we move eny of them far away, they will no longer be the closes points.\n",
    "\n",
    "Thus, the median is stable or robust - a small group of outliers can't affect it.\n",
    "\n",
    "On the other hand we only extract information from the \"middle points\". In fact, if we treat the median as the minimizer of the absolute valued distance, all points between the middle two yield equally low loss, and the precision of estimation is proportional to the spacing between points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysuav1uIJiDN"
   },
   "source": [
    "The plots below show the historgrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jj8w2HMVCqeh"
   },
   "outputs": [],
   "source": [
    "num_repetitions = 10000\n",
    "sample_size = 100\n",
    "\n",
    "sample_gauss = np.random.randn(num_repetitions, sample_size)\n",
    "sample_laplace = np.random.laplace(size=(num_repetitions, sample_size))\n",
    "sample_student = np.random.standard_t(df=2, size=(num_repetitions, sample_size))\n",
    "\n",
    "bins = np.linspace(-1, 1, 101)\n",
    "\n",
    "mean_gauss = sample_gauss.mean(-1)\n",
    "median_gauss = np.median(sample_gauss, axis=-1)\n",
    "\n",
    "mean_laplace = sample_laplace.mean(-1)\n",
    "median_laplace = np.median(sample_laplace, axis=-1)\n",
    "\n",
    "mean_student = sample_student.mean(-1)\n",
    "median_student = np.median(sample_student, axis=-1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131)\n",
    "plt.hist(median_gauss, bins=bins, label='median', alpha=0.3)\n",
    "plt.hist(mean_gauss, bins=bins, label='mean', alpha=0.3)\n",
    "plt.title('Estimator error for Gaussian data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(median_laplace, bins=bins, label='median', alpha=0.3)\n",
    "plt.hist(mean_laplace, bins=bins, label='mean', alpha=0.3)\n",
    "plt.title('Estimator error for Laplace data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(median_student, bins=bins, label='median', alpha=0.3)\n",
    "plt.hist(mean_student, bins=bins, label='mean', alpha=0.3)\n",
    "plt.title('Estimator error for Student-t data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g1lzIr0vsfu"
   },
   "outputs": [],
   "source": [
    "num_repetitions = 1000\n",
    "results = []\n",
    "for sample_size in np.logspace(1, 4, 31):\n",
    "    sample_size = int(np.round(sample_size))\n",
    "    sample_gauss = np.random.randn(num_repetitions, sample_size)\n",
    "    sample_laplace = np.random.laplace(size=(num_repetitions, sample_size))\n",
    "\n",
    "    mean_gauss = sample_gauss.mean(-1)\n",
    "    median_gauss = np.median(sample_gauss, axis=-1)\n",
    "\n",
    "    mean_laplace = sample_laplace.mean(-1)\n",
    "    median_laplace = np.median(sample_laplace, axis=-1)\n",
    "\n",
    "    results.append({'sample_size': sample_size, \n",
    "                    'data_distribution': 'Gaussian',\n",
    "                    'mean': mean_gauss.std(),\n",
    "                    'median': median_gauss.std(),\n",
    "    })\n",
    "    results.append({'sample_size': sample_size, \n",
    "                    'data_distribution': 'Laplace',\n",
    "                    'mean': mean_laplace.std(),\n",
    "                    'median': median_laplace.std(),\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_bKWL6WFNTE"
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    plot = sns.relplot(\n",
    "        data=results.melt(['sample_size', 'data_distribution']), \n",
    "        x='sample_size', y='value', hue='variable', col='data_distribution')\n",
    "    plot.set(xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzgsNKa-kQgQ"
   },
   "source": [
    "In Homerwork 1 we have seen that:\n",
    "- the mean is the MLE estimator for the location of a normally distributed random variable, i.e.\n",
    "\n",
    "  $$\n",
    "    p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2},\n",
    "  $$\n",
    "\n",
    "- the median is the MLE extimator for the location of a laplace random variable, i.e.\n",
    "\n",
    "  $$\n",
    "    p(x) = \\frac{1}{2b}e^{-\\frac{|x-\\mu|}{b}}.\n",
    "  $$\n",
    "\n",
    "Moreover:\n",
    "- assumption of normally distributed errors yields a least squares cost, also called L2 norm,\n",
    "- assumption of laplace distributed errors yields a sum-of-absolute values cost, also called L1 norm.\n",
    "\n",
    "The two cost functions will behave in a radically different way in the presence of outliers: the L2 cost will put a large weight to outliers, while the L1 cost will mostly ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnHzPoyWv7Ly"
   },
   "source": [
    "We can get intuitions about the behavior of Normal and Laplace random variables by looking at their probability mass functions on a logarithmic scale.\n",
    "\n",
    "The normal distribution has a fairly large probability mass near its mean, then quickly rolls down. Intuitively, observing a very large value far away from the mean is improbable, seeing one has a large influence on the MLE estimator (mean).\n",
    "\n",
    "On the other hand the Laplace has \"fatter tails\" - the probability of seeing a large value doesnt decrease as fast. Intuitively, this means that seeing a very larg value, far away from its mean is not that improbable, and its influence on the MLE estimator (median) will not be that large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bW3vEssuwF1H"
   },
   "outputs": [],
   "source": [
    "xx=np.linspace(-10, 10, 101)\n",
    "plt.semilogy(xx, sstats.norm.pdf(xx), label='Normal')\n",
    "plt.semilogy(xx, sstats.laplace.pdf(xx), label='Laplace')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RexWUi3rwoh3"
   },
   "source": [
    "The difference is also visible when we look at the derivatives of the neg log-likelihoods (L1 and L2 norms) (we ignore that absolute value is not differentialbe at 0).\n",
    "\n",
    "On the plot we can see, that no matter how large the error is, the L1 loss \"pushes\" with the same force, while the L2 loss pushes more for larger errrors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3FekeUdwnKH"
   },
   "outputs": [],
   "source": [
    "plt.plot(xx, xx, label='L2 norm derivative')\n",
    "plt.plot(xx, np.sign(xx) / np.abs(np.sign(xx)), # this division will insert a NaN for xx=0\n",
    "         label='L1 norm derivative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2rNOTyAldvk"
   },
   "source": [
    "### Huber loss, or the best of both worlds\n",
    "\n",
    "When there are no outliers, the mean's covergence to the location of the underlying probability distribution is faster than the convergence of median.\n",
    "\n",
    "Intuitively, to keep the \"best of both workds\" we would want to reject the outliers, then use the inliners to compute the mean. \n",
    "\n",
    "The [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) tries to do this automatically - it ressembles a square function for small deviations, and the sbsolute value for large ones:\n",
    "\n",
    "$$\n",
    "L_\\delta (a) = \\begin{cases}\n",
    " \\frac{1}{2}{a^2}                   & \\text{for } |a| \\le \\delta, \\\\\n",
    " \\delta (|a| - \\frac{1}{2}\\delta), & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "It is a piecewise function, in the $\\delta$ radius it is quadratic, out f it it grows linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1nnEt7io07D"
   },
   "source": [
    "### The pseudo-Huber loss\n",
    "\n",
    "The Huber loss is a piecewise function, making it non-smooth (the second derivative is not contigupous at the splicing points). A smooth approximation is:\n",
    "$$\n",
    "\\delta^2\\left(\\sqrt{\\left(\\frac{x}{\\delta}\\right)^2 + 1} - 1\\right)\n",
    "$$\n",
    "\n",
    "Below we plot L2 loss long with Huber and pseudo-Huber:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI3bbS5HoR68"
   },
   "outputs": [],
   "source": [
    "def L(a, delta):\n",
    "    return np.where(np.abs(a) <= delta, \n",
    "                    0.5*a**2, \n",
    "                    delta*(np.abs(a) - 0.5 * delta))\n",
    "\n",
    "x = np.linspace(-4, 4, 100)\n",
    "plt.plot(x, 0.5 * x**2, label='$x^2/2$')\n",
    "plt.plot(x, L(x, 1), label='$L_{\\delta=1}(x)$')\n",
    "plt.plot(x, np.sqrt(x**2 + 1) - 1, label=r'pseudo-$L_{\\delta=1}(x)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr45ztS0royU"
   },
   "source": [
    "### Demonstration\n",
    "\n",
    "We will now demonstrate the behavior of linear and Huber regression in the presence of outliers. Please note that programming assignment 2 compares linear regression to L1 regression, and introduces its generalization called \"quantile regression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcj4VfMqrOtf"
   },
   "outputs": [],
   "source": [
    "# We repeat here code from Lecture 02!\n",
    "\n",
    "def gen_data(N=10, sigma_noise=.2, fun='poly2'):\n",
    "    X = 2 * np.random.rand(N) - 1.0\n",
    "    # sort the data \n",
    "    X = np.sort(X).reshape(-1, 1)\n",
    "    eps = np.random.randn(N, 1) * sigma_noise\n",
    "    if fun=='poly1':\n",
    "        Y = 0.3 + .7*X + eps\n",
    "    elif fun=='poly2':\n",
    "        Y = 0.3 + .7*X -1.1*X**2 + eps\n",
    "    elif fun =='log':\n",
    "        Y = np.log(X+1)**.5 + eps\n",
    "    elif fun == 'sin':\n",
    "        Y = np.sin(3*np.pi*X) + eps\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fun type: {fun}.\")\n",
    "    if sorted:\n",
    "        xind = np.argsort(X[0, :])\n",
    "        X= X[:, xind]\n",
    "        Y= Y[:, xind]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def expand(x, d):\n",
    "    powers = np.arange(d + 1).reshape(1, -1)\n",
    "    # Hint: if you don't understand this code, read about broadcasting!\n",
    "    x_expanded = x.reshape(-1, 1) ** powers\n",
    "    return x_expanded\n",
    "\n",
    "\n",
    "class PolyRegression(sklearn.linear_model.LinearRegression):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        # We don't fit the intercept, because we have explicitely added the 0-th power\n",
    "        # of X to the expended representation.\n",
    "        super().__init__(fit_intercept=False)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return super().fit(expand(x, self.degree), y) \n",
    "\n",
    "    def predict(self, x):\n",
    "        return super().predict(expand(x, self.degree)) \n",
    "\n",
    "\n",
    "class PolyHuberRegression(sklearn.linear_model.HuberRegressor):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "        # We don't fit the intercept, because we have explicitely added the 0-th power\n",
    "        # of X to the expended representation.\n",
    "        super().__init__(fit_intercept=False)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return super().fit(expand(x, self.degree), y) \n",
    "\n",
    "    def predict(self, x):\n",
    "        return super().predict(expand(x, self.degree)) \n",
    "\n",
    "\n",
    "XC,YC = gen_data(N=30, sigma_noise=0.2, fun='poly1')\n",
    "# now append some outliers\n",
    "\n",
    "XO = np.concatenate([XC, [[-1], [-0.9], [-0.75]]], axis=0)\n",
    "YO = np.concatenate([YC, [[3], [3], [3]]], axis=0)\n",
    "fig = plt.figure()\n",
    "plt.plot(XO, YO, 'o') #note: plot assumes that each column is a data series\n",
    "plt.title('X-Y relation with outliers')\n",
    "\n",
    "\n",
    "degree = 1\n",
    "\n",
    "plt.plot(XC, \n",
    "         PolyRegression(degree).fit(XC, YC).predict(XC), \n",
    "         '-', label=f'linear regression, no outliers')\n",
    "\n",
    "plt.plot(XC, \n",
    "         PolyRegression(degree).fit(XO, YO).predict(XC), \n",
    "         '-', label=f'linear regression, with outliers')\n",
    "\n",
    "plt.plot(XC, \n",
    "         PolyHuberRegression(degree).fit(XC, YC.ravel()).predict(XC), \n",
    "         '-', label=f'huber regression, no outliers')\n",
    "\n",
    "plt.plot(XC, \n",
    "         PolyHuberRegression(degree).fit(XO, YO.ravel()).predict(XC), \n",
    "         '-', label=f'huber regression, with outliers')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVvMKIJ_szxJ"
   },
   "source": [
    "## A universal loss function\n",
    "\n",
    "Recently, a family of loss function that generalizes the Huber loss, and other robust losses was presented. Programming assignment has a bonus problem, please see the [paper](https://arxiv.org/abs/1701.03077) and [video](https://www.youtube.com/watch?v=BmNKbnF69eY)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fLwn6t7uqk3"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The probabilistic approach to machine learning gave us two powerful tools:\n",
    "1. First, the MLE principle gives us a principled way to derive loss function based on assumptions about the conditional data distribution. We have used the MLE to derive loss functions for classifiers and regressors, under different assumptions about the error distribution.\n",
    "2. Second, understanding the models and their assumptions allows us to verify whether we are using the right tool for the problem at hand, and to understand the influence of data errors, outliers, and the mismatch between the model and the real world."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04-notes-full.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
